{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ObjectTechnologySolutionsIndia-OTSI/LLM-Trail/blob/Rajeshwari/Quickstart_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmE76hXMmnVK",
        "outputId": "bc5f640d-7e3a-4d80-ea8a-daa747f1e30e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.6.1 typing-extensions-4.9.0\n"
          ]
        }
      ],
      "source": [
        "#Install OpenAI\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p461TDBSp888"
      },
      "outputs": [],
      "source": [
        "#To integrate OpenAI features into your project, include the following imports: import openai for accessing OpenAI functionality and import os for configuring the API key.\n",
        "import openai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxg5KSA2qxN-"
      },
      "outputs": [],
      "source": [
        "#To access the value of the 'OPENAI_API_KEY' environment variable using the os module.\n",
        "os.environ['OPENAI_API_KEY'] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iXmC-CjrGPu",
        "outputId": "7c585a4f-53a0-466a-a15c-23a07e9bc9cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in a small village in India, there lived a young girl named Radha. Radha was bright, curious, and full of dreams. She had a innate desire to bring positive change in the world, and she was determined to make a difference.\n",
            "\n",
            "Growing up in a humble household, Radha was inspired by her parents' selflessness and their commitment to education. Her father worked as a school teacher, and her mother dedicated her days to supporting the local community. They often emphasized the value of education and compassion, teaching Radha that knowledge is a powerful tool to change lives.\n",
            "\n",
            "As Radha grew older, her passion for learning expanded. She immersed herself in books, soaking up knowledge about society, history, and various cultures. However, Radha's education was constantly interrupted due to her family's financial struggles. Despite the challenges, she remained undeterred, knowing that education was the key to her dreams.\n",
            "\n",
            "One day, Radha came across a newspaper article about a scholarship offered by a renowned philanthropist named Sudha Murthy. Sudha was known for her dedication to social causes and her commitment to empowering young minds through education. Filled with hope and determination, Radha applied for the scholarship, pouring her heart out in the application letter.\n",
            "\n",
            "Days turned into weeks, and weeks turned into months. Radha's anticipation grew as she awaited a response. Finally, one sunny morning, a letter arrived on her doorstep with the words \"Congratulations, you have been selected!\" written on it. Overwhelmed with joy, Radha couldn't believe her luck. This scholarship was not just a financial support, but a recognition of her potential and her dreams.\n",
            "\n",
            "With Sudha Murthy's scholarship, Radha pursued her education relentlessly. She worked hard, excelling in her studies, and actively participating in various social initiatives. Inspired by Sudha Murthy's philanthropy, Radha began volunteering in her community, teaching underprivileged children and assisting local charities.\n",
            "\n",
            "Years later, Radha completed her education and became a successful social entrepreneur. She founded an organization dedicated to educating underprivileged children, ensuring that no child was left behind due to their financial background. Radha's efforts were recognized nationally, and she was honored with an award for her outstanding contribution to society.\n",
            "\n",
            "During the award ceremony, Radha had the privilege of meeting Sudha Murthy herself. The two sat down together, sharing stories and exchanging ideas. Sudha Murthy was deeply touched by Radha's journey and the impact she had made. She looked at Radha with pride and said, \"You are the epitome of the change I believe in. Keep inspiring, and never stop making a difference.\"\n",
            "\n",
            "Radha's journey, like Sudha Murthy's, became a symbol of hope and inspiration for countless other young individuals who aspired to make a positive change. It reminded them that no matter the circumstances, determination, hard work, and compassion could fuel their dreams and transform lives.\n",
            "\n",
            "And so, the story of Radha and Sudha Murthy's scholarship became a legend that would be shared from one generation to another, reminding everyone of the power an education and a helping hand can hold in shaping a better tomorrow.\n"
          ]
        }
      ],
      "source": [
        "#Example-1\n",
        "#Using GPT-3.5-turbo model, to generate a short story.\n",
        "#Import openai library and initialize openai client\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert story teller\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a short story assuming yourself as sudha murthy\"}\n",
        "  ],\n",
        ")\n",
        "#To retrieve and Print the Generated Story\n",
        "message=completion.choices[0].message\n",
        "expanded_text=message.content.expandtabs(tabsize=4)\n",
        "print(expanded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1rkhJ6AsFbg",
        "outputId": "37848342-183e-44ae-dddd-26a0d6f8a655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She did not go to the market.\n"
          ]
        }
      ],
      "source": [
        "#Example-2\n",
        "#Using GPT-3.5-turbo model to Convert ungrammatical statements into standard English.\n",
        "#Import openai library and initialize openai client\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with statements, and your task is to convert them to standard English.\"},\n",
        "    {\"role\": \"user\", \"content\": \"She no went to the market.\"}\n",
        "  ]\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text=message.content.expandtabs(tabsize=4)\n",
        "print(expanded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfe8zEc7z4Wq",
        "outputId": "bf80da7b-d2c6-4dd1-9dd9-9b8432798f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jupiter is a really big planet in our Solar System. It is the fifth planet from the Sun and it is the largest planet. Jupiter is made mostly of gas and it is much\n",
            "smaller than the Sun but much bigger than all the other planets combined. You can see Jupiter in the night sky because it is really bright. It has been known to\n",
            "people for a very long time and it is named after a Roman god. When we look at Jupiter from Earth, it can be so bright that it can make shadows. It is usually the\n",
            "third-brightest thing we can see at night, after the Moon and Venus.\n"
          ]
        }
      ],
      "source": [
        "#Example-3\n",
        "#Using GPT-3.5-turbo model for text summarisation.\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Summarize content you are provided with for a second-grade student.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.\"}\n",
        "  ], max_tokens=500, temperature=0.7,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpFatbCW9A-t",
        "outputId": "31bcfd50-d39b-4942-e473-513bfbc2927d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "😂\n"
          ]
        }
      ],
      "source": [
        "#Example-4\n",
        "#Using GPT-3.5-turbo model for emoji translation\n",
        "#Import openai library and initialize openai client\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with text, and your task is to translate it into emojis. Do not use any regular text. Do your best with emojis only.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Laugh out loud\"}\n",
        "  ], max_tokens=50, temperature=0.2,top_p=0.5\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW88fRTn1hrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80060956-0e4f-4906-db0a-3d514072ecb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This code defines a Log class that allows for logging events to a file and retrieving the state of the log. The constructor initializes the log file by creating the\n",
            "necessary directories and opening the file in \"a+\" mode (append and read). It also checks if the file is newline-terminated and adds a newline if necessary.  The log\n",
            "method takes an event as input, assigns a unique ID to the event, and writes the event as a JSON object to the log file. It also adds a newline character after each\n",
            "event.  The state method reads the log file line by line, parses each line as a JSON object, and checks if the event type is \"submit\" and if it was successful. It\n",
            "keeps track of the set of completed events and the last event in the log, and returns this information as a dictionary.\n"
          ]
        }
      ],
      "source": [
        "#Example-5\n",
        "#Using GPT-3.5-turbo model for Code explanation.\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with a piece of code, and your task is to explain it in a concise way.\"},\n",
        "    {\"role\": \"user\", \"content\": \"class Log:\\n def __init__(self, path):\\n dirname = os.path.dirname(path)\\n  os.makedirs(dirname, exist_ok=True)\\n f = open(path, \\\"a+\\\")\\n    \\n            # Check that the file is newline-terminated\\n            size = os.path.getsize(path)\\n            if size > 0:\\n                f.seek(size - 1)\\n                end = f.read(1)\\n                if end != \\\"\\\\n\\\":\\n                    f.write(\\\"\\\\n\\\")\\n            self.f = f\\n            self.path = path\\n    \\n        def log(self, event):\\n            event[\\\"_event_id\\\"] = str(uuid.uuid4())\\n            json.dump(event, self.f)\\n            self.f.write(\\\"\\\\n\\\")\\n    \\n        def state(self):\\n            state = {\\\"complete\\\": set(), \\\"last\\\": None}\\n            for line in open(self.path):\\n                event = json.loads(line)\\n                if event[\\\"type\\\"] == \\\"submit\\\" and event[\\\"success\\\"]:\\n                    state[\\\"complete\\\"].add(event[\\\"id\\\"])\\n                    state[\\\"last\\\"] = event\\n            return state\"\n",
        "    }\n",
        "     ], temperature=0.7,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-6\n",
        "#Using GPT-3.5-turbo model for bug fixation.\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with a piece of Python code, and your task is to find and fix bugs in it.\"},\n",
        "    {\"role\": \"user\", \"content\": \"import Random\\n    a = random.randint(1,12)\\n    b = random.randint(1,12)\\n    for i in range(10):\\n        question = \\\"What is \\\"+a+\\\" x \\\"+b+\\\"? \\\"\\n        answer = input(question)\\n        if answer = a*b\\n            print (Well done!)\\n        else:\\n            print(\\\"No.\\\")\"\n",
        "    }\n",
        "     ], temperature=0.7,top_p=0.7,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EB2iGUq6TL8",
        "outputId": "abb47700-a4cc-4064-e006-7c5c3d46dc0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the corrected code:  ```python import random  a = random.randint(1, 12) b = random.randint(1, 12)  for i in range(10):     question = \"What is \" + str(a) + \"\n",
            "x \" + str(b) + \"? \"     answer = int(input(question))          if answer == a * b:         print(\"Well done!\")     else:         print(\"No.\") ```  Changes made: 1.\n",
            "The `\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-7\n",
        "#Using GPT-3.5-turbo model for tweet classification.\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with a tweet, and your task is to classify its sentiment as positive, neutral, or negative.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I don't dislike pizza\"\n",
        "    }\n",
        "     ], temperature=0.7,top_p=0.7,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTp2-Mdi8aNr",
        "outputId": "01ff0eb5-e524-4f2c-d159-671887090bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-10\n",
        "#Using GPT-3.5-turbo model for reasoning by using Chain of thought prompting\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert in anwering reasoning questions.\"},\n",
        "    {\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1\"},\n",
        "{\"role\": \"assistant\", \"content\": \"Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\"},\n",
        "{\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\"},\n",
        "{\"role\": \"assistant\", \"content\": \"Adding all the odd numbers (17, 19) gives 36. The answer is True.\"},\n",
        "{\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\"},\n",
        "{\"role\": \"assistant\", \"content\": \"Adding all the odd numbers (11, 13) gives 24. The answer is True.\"},\n",
        "{\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\"},\n",
        "{\"role\": \"assistant\", \"content\": \"Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\"},\n",
        "{\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\"},\n",
        "\n",
        "     ], temperature=0.7,top_p=0.7,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0g-31P3Bi1o",
        "outputId": "69ad0ef5-f6e0-4096-e23a-f55716c5649f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-11\n",
        "#Using GPT-3.5-turbo model to generate interview questions\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Assume yourself as a datascience expert with over 20 years of working experience\"},\n",
        "        {\"role\": \"user\", \"content\": \"Create a list of 8 questions for an interview for the job of a data analyst\"},\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rxKB6G3FOrz",
        "outputId": "a02ab0c2-f7db-4c97-979c-018f2575b555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Can you describe your experience in data analysis and how it aligns with the requirements of this role? 2. How do you approach data cleaning and preprocessing?\n",
            "Can you provide an example of a challenging data cleaning task you have encountered and how you resolved it? 3. What statistical techniques and tools do you\n",
            "typically use to analyze data? Can you provide an example of a project where you applied these techniques to derive meaningful insights? 4. How do you ensure the\n",
            "accuracy and reliability of your data analysis? Can you describe any quality control measures you have implemented in your previous work? 5. Can you explain your\n",
            "approach to data visualization? How do you select the most appropriate visualization techniques to effectively communicate insights to stakeholders? 6. How do you\n",
            "handle large datasets and complex data structures? Can you describe a project where you had to work with big data and the strategies you employed to manage and\n",
            "analyze it efficiently? 7. Can you discuss your experience with predictive modeling and machine learning algorithms? Have you worked on any projects where you built\n",
            "predictive models to solve business problems? What techniques and tools did you use? 8. How do you stay updated with the latest trends and advancements in data\n",
            "analysis? Can you provide examples of any self-learning initiatives you have undertaken to enhance your skills in this field?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-12\n",
        "#Using GPT-3.5-turbo model for generating lesson plan writer for a specific topic\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Assume yourself as a datascience expert with writer skills\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a lesson plan for an induction to machine learning. The lesson plan should cover what,why is machine learning,benefits,steps involved,business understanding,preprocessing,machine learning models and evaluation metrics.\"},\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1jL_WfKGX4M",
        "outputId": "91437c23-8e32-41ea-dfa6-b17ae4fe9ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lesson Plan: Introduction to Machine Learning  Objective: By the end of this lesson, students will have a clear understanding of what machine learning is, its\n",
            "benefits, the steps involved in the machine learning process, and the key components of machine learning models and evaluation metrics.  Duration: 60 minutes\n",
            "Materials: - Presentation slides - Whiteboard or flipchart - Markers  Lesson Outline:  1. Introduction (5 minutes)    - Greet the students and introduce yourself as\n",
            "a data science expert.    - Explain the importance of machine learning in today's world.    - Share some real-life examples of machine learning applications.  2.\n",
            "What is Machine Learning? (10 minutes)    - Define machine learning as a subset of artificial intelligence that enables computers to learn from data without being\n",
            "explicitly programmed.    - Discuss the difference between traditional programming and machine learning.    - Explain the concept of training data and how it is used\n",
            "to build machine learning models.  3. Why Machine Learning? (10 minutes)    - Discuss the benefits of machine learning in various industries, such as healthcare,\n",
            "finance, marketing, and transportation.    - Highlight how machine learning can automate tasks, make predictions, and uncover hidden patterns in large datasets.    -\n",
            "Emphasize the potential impact of machine learning on business growth and decision-making.  4. Steps Involved in Machine Learning (15 minutes)    a. Business\n",
            "Understanding       - Explain the importance of understanding the problem domain and defining clear objectives.       - Discuss how machine learning can help solve\n",
            "specific business problems and improve decision-making processes.     b. Data Preprocessing       - Introduce the concept of data preprocessing and its significance\n",
            "in machine learning.       - Discuss common preprocessing techniques, such as handling missing values, encoding categorical variables, and scaling numerical\n",
            "features.     c. Machine Learning Models       - Provide an overview of popular machine learning algorithms, such as linear regression, decision trees, and support\n",
            "vector machines.       - Explain the concept of supervised and unsupervised learning and their respective use cases.     d. Evaluation Metrics       - Discuss the\n",
            "importance of evaluation metrics in assessing the performance of machine learning models.       - Introduce common evaluation metrics, such as accuracy, precision,\n",
            "recall, and F1 score.       - Explain the concept of overfitting and the need for cross-validation techniques.  5. Recap and Q&A (10 minutes)    - Summarize the key\n",
            "points covered in the lesson.    - Encourage students to ask questions and clarify any doubts they may have.    - Provide additional resources for further learning,\n",
            "such as online courses or books.  6. Conclusion (5 minutes)    - Thank the students for their participation and interest in the topic.    - Highlight the importance\n",
            "of machine learning skills in today's job market.    - Encourage students to explore and practice machine learning techniques in their own projects.  Note: The\n",
            "lesson plan can be adjusted based on the students' prior knowledge and the available time. It is recommended to include interactive activities, examples, and real-\n",
            "world case studies to enhance student engagement and understanding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-13\n",
        "#Using GPT-3.5-turbo model for pros and cons discusser\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "       {\"role\": \"user\", \"content\": \"Analyze the pros and cons of remote work vs. office work\"},\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luY5LmHDMtoi",
        "outputId": "1e3860da-5670-4540-e548-bcaaef5d1b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pros of remote work: 1. Flexibility: Remote work allows employees to have a more flexible schedule, as they can choose when and where they work. This can lead to\n",
            "better work-life balance and increased productivity. 2. Cost savings: Remote work eliminates commuting costs, such as transportation expenses and parking fees. It\n",
            "also reduces expenses related to office attire, meals, and other work-related expenses. 3. Increased job opportunities: Remote work allows employees to work for\n",
            "companies located anywhere in the world, expanding their job opportunities beyond their local area. 4. Reduced distractions: Working remotely can minimize\n",
            "distractions that are common in office environments, such as interruptions from colleagues, office noise, and unnecessary meetings. 5. Improved focus: Some\n",
            "individuals find it easier to concentrate and be more productive in a remote work environment, as they have more control over their surroundings and can create a\n",
            "personalized workspace.  Cons of remote work: 1. Lack of social interaction: Remote work can be isolating, as employees may miss out on the social interactions and\n",
            "camaraderie that come with working in an office. This can lead to feelings of loneliness and decreased motivation. 2. Communication challenges: Remote work can make\n",
            "communication more difficult, as it relies heavily on technology. Misunderstandings and delays in communication can occur, especially when dealing with complex or\n",
            "sensitive topics. 3. Limited access to resources: Remote workers may not have the same access to office resources, such as specialized equipment, software, or\n",
            "physical documents. This can hinder their ability to perform certain tasks efficiently. 4. Blurred work-life boundaries: Working remotely can make it challenging to\n",
            "separate work and personal life, as the physical boundaries between the two become less defined. This can lead to longer working hours and increased stress. 5.\n",
            "Potential for decreased visibility and career growth: Remote workers may have limited opportunities for face-to-face interactions with managers and colleagues, which\n",
            "can impact their visibility within the organization. This may result in fewer chances for career advancement or recognition.  Pros of office work: 1. Social\n",
            "interaction and collaboration: Working in an office allows for face-to-face interactions with colleagues, fostering teamwork, idea sharing, and a sense of community.\n",
            "2. Access to resources and infrastructure: Office work provides employees with access to specialized equipment, software, and resources that may not be available at\n",
            "home. This can enhance productivity and efficiency. 3. Clear work-life boundaries: Having a physical separation between work and personal life can help individuals\n",
            "establish clear boundaries, allowing for better work-life balance. 4. Opportunities for mentorship and career growth: Being present in the office can provide more\n",
            "opportunities for mentorship, networking, and career advancement, as employees have more visibility and exposure to senior colleagues and managers. 5. Enhanced\n",
            "communication: In-person communication can be more effective and efficient, as it allows for immediate feedback, non-verbal cues, and better understanding of complex\n",
            "or sensitive topics.  Cons of office work: 1. Commuting and associated costs: Office work often requires commuting, which can be time-consuming and expensive,\n",
            "especially in urban areas with heavy traffic or limited public transportation options. 2. Distractions and interruptions: Office environments can be noisy and filled\n",
            "with distractions, such as colleagues' conversations, phone calls, or impromptu meetings, which can hinder concentration and productivity. 3. Lack of flexibility:\n",
            "Office work typically follows a fixed schedule, leaving employees with less flexibility to manage personal commitments or unexpected events. 4. Higher expenses:\n",
            "Office work can lead to higher expenses, including transportation costs, office attire, meals, and other work-related expenses. 5. Health and safety concerns: Office\n",
            "environments may pose health and safety risks, such as exposure to contagious illnesses, ergonomic issues, or workplace accidents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-14\n",
        "#Using GPT-3.5-turbo model for review classifier\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You will be presented with user reviews and your job is to provide a set of tags from the following list. Provide your answer in bullet point form. Choose ONLY from the list of tags provided here (choose either the positive or the negative tag but NOT both):\\n    \\n    - Provides good value for the price OR Costs too much\\n    - Works better than expected OR Did not work as well as expected\\n    - Includes essential features OR Lacks essential features\\n    - Easy to use OR Difficult to use\\n    - High quality and durability OR Poor quality and durability\\n    - Easy and affordable to maintain or repair OR Difficult or costly to maintain or repair\\n    - Easy to transport OR Difficult to transport\\n    - Easy to store OR Difficult to store\\n    - Compatible with other devices or systems OR Not compatible with other devices or systems\\n    - Safe and user-friendly OR Unsafe or hazardous to use\\n    - Excellent customer support OR Poor customer support\\n    - Generous and comprehensive warranty OR Limited or insufficient warranty\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"I recently purchased the Inflatotron 2000 airbed for a camping trip and wanted to share my experience with others. Overall, I found the airbed to be a mixed bag with some positives and negatives.\\n    \\n    Starting with the positives, the Inflatotron 2000 is incredibly easy to set up and inflate. It comes with a built-in electric pump that quickly inflates the bed within a few minutes, which is a huge plus for anyone who wants to avoid the hassle of manually pumping up their airbed. The bed is also quite comfortable to sleep on and offers decent support for your back, which is a major plus if you have any issues with back pain.\\n    \\n    On the other hand, I did experience some negatives with the Inflatotron 2000. Firstly, I found that the airbed is not very durable and punctures easily. During my camping trip, the bed got punctured by a stray twig that had fallen on it, which was quite frustrating. Secondly, I noticed that the airbed tends to lose air overnight, which meant that I had to constantly re-inflate it every morning. This was a bit annoying as it disrupted my sleep and made me feel less rested in the morning.\\n    \\n    Another negative point is that the Inflatotron 2000 is quite heavy and bulky, which makes it difficult to transport and store. If you're planning on using this airbed for camping or other outdoor activities, you'll need to have a large enough vehicle to transport it and a decent amount of storage space to store it when not in use.\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLyX3X8iNxaZ",
        "outputId": "302a763c-a874-4a96-cefe-3d337a43763a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Easy to use - High quality and durability - Difficult to transport - Difficult to store\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-15\n",
        "#Using GPT-3.5-turbo model for Meeting notes summarizer\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "   {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You will be provided with meeting notes, and your task is to summarize the meeting as follows:\\n    \\n    -Overall summary of discussion\\n    -Action items (what needs to be done and who is doing it)\\n    -If applicable, a list of topics that need to be discussed more fully in the next meeting.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Meeting Date: March 5th, 2050\\n    Meeting Time: 2:00 PM\\n    Location: Conference Room 3B, Intergalactic Headquarters\\n    \\n    Attendees:\\n    - Captain Stardust\\n    - Dr. Quasar\\n    - Lady Nebula\\n    - Sir Supernova\\n    - Ms. Comet\\n    \\n    Meeting called to order by Captain Stardust at 2:05 PM\\n    \\n    1. Introductions and welcome to our newest team member, Ms. Comet\\n    \\n    2. Discussion of our recent mission to Planet Zog\\n    - Captain Stardust: \\\"Overall, a success, but communication with the Zogians was difficult. We need to improve our language skills.\\\"\\n    - Dr. Quasar: \\\"Agreed. I'll start working on a Zogian-English dictionary right away.\\\"\\n    - Lady Nebula: \\\"The Zogian food was out of this world, literally! We should consider having a Zogian food night on the ship.\\\"\\n    \\n    3. Addressing the space pirate issue in Sector 7\\n    - Sir Supernova: \\\"We need a better strategy for dealing with these pirates. They've already plundered three cargo ships this month.\\\"\\n    - Captain Stardust: \\\"I'll speak with Admiral Starbeam about increasing patrols in that area.\\n    - Dr. Quasar: \\\"I've been working on a new cloaking technology that could help our ships avoid detection by the pirates. I'll need a few more weeks to finalize the prototype.\\\"\\n    \\n    4. Review of the annual Intergalactic Bake-Off\\n    - Lady Nebula: \\\"I'm happy to report that our team placed second in the competition! Our Martian Mud Pie was a big hit!\\\"\\n    - Ms. Comet: \\\"Let's aim for first place next year. I have a secret recipe for Jupiter Jello that I think could be a winner.\\\"\\n    \\n    5. Planning for the upcoming charity fundraiser\\n    - Captain Stardust: \\\"We need some creative ideas for our booth at the Intergalactic Charity Bazaar.\\\"\\n    - Sir Supernova: \\\"How about a 'Dunk the Alien' game? We can have people throw water balloons at a volunteer dressed as an alien.\\\"\\n    - Dr. Quasar: \\\"I can set up a 'Name That Star' trivia game with prizes for the winners.\\\"\\n    - Lady Nebula: \\\"Great ideas, everyone. Let's start gathering the supplies and preparing the games.\\\"\\n    \\n    6. Upcoming team-building retreat\\n    - Ms. Comet: \\\"I would like to propose a team-building retreat to the Moon Resort and Spa. It's a great opportunity to bond and relax after our recent missions.\\\"\\n    - Captain Stardust: \\\"Sounds like a fantastic idea. I'll check the budget and see if we can make it happen.\\\"\\n    \\n    7. Next meeting agenda items\\n    - Update on the Zogian-English dictionary (Dr. Quasar)\\n    - Progress report on the cloaking technology (Dr. Quasar)\\n    - Results of increased patrols in Sector 7 (Captain Stardust)\\n    - Final preparations for the Intergalactic Charity Bazaar (All)\\n    \\n    Meeting adjourned at 3:15 PM. Next meeting scheduled for March 19th, 2050 at 2:00 PM in Conference Room 3B, Intergalactic Headquarters.\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7,frequency_penalty=0.5\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9fQkGqiQ5zz",
        "outputId": "ce7096a2-9852-4fcf-8dc9-36d4cc908276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Summary of Discussion: The meeting covered various topics including a discussion of the recent mission to Planet Zog, addressing the space pirate issue in\n",
            "Sector 7, reviewing the annual Intergalactic Bake-Off, planning for the upcoming charity fundraiser, and discussing an upcoming team-building retreat.   Action\n",
            "Items: 1. Dr. Quasar will work on creating a Zogian-English dictionary to improve communication skills with the Zogians. 2. Captain Stardust will speak with Admiral\n",
            "Starbeam about increasing patrols in Sector 7 to address the space pirate issue. 3. Dr. Quasar will finalize the prototype of a new cloaking technology that could\n",
            "help ships avoid detection by pirates. 4. The team will gather supplies and prepare games for their booth at the Intergalactic Charity Bazaar. 5. Captain Stardust\n",
            "will check the budget to see if a team-building retreat to the Moon Resort and Spa is feasible.  Topics for Next Meeting: 1. Update on the progress of the Zogian-\n",
            "English dictionary by Dr. Quasar. 2. Progress report on the cloaking technology by Dr. Quasar. 3. Results of increased patrols in Sector 7 reported by Captain\n",
            "Stardust. 4. Final preparations for the Intergalactic Charity Bazaar discussed by all.  Next Meeting: March 19th, 2050 at 2:00 PM in Conference Room 3B,\n",
            "Intergalactic Headquarters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-16\n",
        "#Using GPT-3.5-turbo model for Natural Language to SQL\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "   {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"Given the following SQL tables, your job is to write queries given a user’s request.\\n    \\n    CREATE TABLE Orders (\\n      OrderID int,\\n      CustomerID int,\\n      OrderDate datetime,\\n      OrderTime varchar(8),\\n      PRIMARY KEY (OrderID)\\n    );\\n    \\n    CREATE TABLE OrderDetails (\\n      OrderDetailID int,\\n      OrderID int,\\n      ProductID int,\\n      Quantity int,\\n      PRIMARY KEY (OrderDetailID)\\n    );\\n    \\n    CREATE TABLE Products (\\n      ProductID int,\\n      ProductName varchar(50),\\n      Category varchar(50),\\n      UnitPrice decimal(10, 2),\\n      Stock int,\\n      PRIMARY KEY (ProductID)\\n    );\\n    \\n    CREATE TABLE Customers (\\n      CustomerID int,\\n      FirstName varchar(50),\\n      LastName varchar(50),\\n      Email varchar(100),\\n      Phone varchar(20),\\n      PRIMARY KEY (CustomerID)\\n    );\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Write a SQL query which computes the average total order value for all orders on 2023-04-01.\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW2WzFw1RU2_",
        "outputId": "95ab8d63-0d6c-47b2-a92a-5b80c4027e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT AVG(total_order_value) AS average_order_value FROM (   SELECT o.OrderID, SUM(p.UnitPrice * od.Quantity) AS total_order_value   FROM Orders o   JOIN\n",
            "OrderDetails od ON o.OrderID = od.OrderID   JOIN Products p ON od.ProductID = p.ProductID   WHERE o.OrderDate = '2023-04-01'   GROUP BY o.OrderID ) AS subquery;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-17\n",
        "#Using GPT-3.5-turbo model for Language translation\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "   {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You will be provided with a sentence in English, and your task is to translate it into German.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Languaage translation is the most booming topic in current situation\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAgpdSNlR78F",
        "outputId": "63965f50-10a8-485e-92c7-25416c055cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sprachübersetzung ist das derzeit boomendste Thema.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-18\n",
        "#Using GPT-3.5-turbo model for improving code efficiency\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "   {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You will be provided with a piece of Python code, and your task is to provide ideas for efficiency improvements.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"from typing import List\\n def has_sum_k(nums: List[int], k: int) -> bool:\\n        \\\"\\\"\\\"\\n        Returns True if there are two distinct elements in nums such that their sum \\n        is equal to k, and otherwise returns False.\\n        \\\"\\\"\\\"\\n        n = len(nums)\\n        for i in range(n):\\n            for j in range(i+1, n):\\n                if nums[i] + nums[j] == k:\\n                    return True\\n        return False\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnl-Lt6sTUCW",
        "outputId": "f484bf82-dcab-4393-d9d0-bb3e23998a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are a few efficiency improvements that can be made to the code:  1. Use a set to store the elements of the list: Instead of using a list, you can use a set to\n",
            "store the elements of the list. This will reduce the time complexity of checking if an element exists in the list from O(n) to O(1).  2. Use a single loop instead of\n",
            "nested loops: Instead of using nested loops to check all possible pairs of elements, you can use a single loop and check if the difference between the current\n",
            "element and k is present in the set. This will reduce the time complexity from O(n^2) to O(n).  3. Use early termination: If a pair of elements is found whose sum is\n",
            "equal to k, you can immediately return True instead of continuing the loop. This will improve the average case time complexity.  Here's an updated version of the\n",
            "code incorporating these improvements:  ```python from typing import List  def has_sum_k(nums: List[int], k: int) -> bool:     \"\"\"     Returns True if there are two\n",
            "distinct elements in nums such that their sum      is equal to k, and otherwise returns False.     \"\"\"     num_set = set(nums)     for num in nums:         if k -\n",
            "num in num_set and k - num != num:             return True     return False ```  These improvements will significantly reduce the time complexity of the code.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-19\n",
        "#Using GPT-3.5-turbo model for writing a function from the specification\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"write a python code which takes 3 numbers as input and finds the greatest of them \"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e5yw7fmU6jc",
        "outputId": "c6523aaf-9504-4ff8-eb8e-8b25ceee4b74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a Python code that takes 3 numbers as input and finds the greatest of them:\n",
            "\n",
            "```python\n",
            "# Taking input from the user\n",
            "num1 = float(input(\"Enter the first number: \"))\n",
            "num2 = float(input(\"Enter the second number: \"))\n",
            "num3 = float(input(\"Enter the third number: \"))\n",
            "\n",
            "# Comparing the numbers to find the greatest\n",
            "if num1 >= num2 and num1 >= num3:\n",
            "    greatest = num1\n",
            "elif num2 >= num1 and num2 >= num3:\n",
            "    greatest = num2\n",
            "else:\n",
            "    greatest = num3\n",
            "\n",
            "# Printing the greatest number\n",
            "print(\"The greatest number is:\", greatest)\n",
            "```\n",
            "\n",
            "In this code, we first take input from the user for three numbers using the `input()` function. We convert the input to float using the `float()` function to handle decimal numbers.\n",
            "\n",
            "Then, we compare the numbers using if-elif-else statements to find the greatest number. Finally, we print the greatest number using the `print()` function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-20\n",
        "#Using GPT-3.5-turbo model for text completion\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"Complete the text that will be provided to you\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Machine learning is a\"\n",
        "    }\n",
        "     ], temperature=0.8,top_p=0.1,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYb_wIaWV2mi",
        "outputId": "cbaf5281-acbd-478d-c0db-cbf1642f9486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subfield of artificial intelligence that focuses on the development of algorithms and models that allow computers to learn and make predictions or decisions without being explicitly programmed. It involves the use of statistical techniques and data analysis to enable machines to learn from and adapt to new information. Machine learning algorithms can be trained on large datasets to recognize patterns, make predictions, or solve complex problems. This technology has applications in various fields, including image and speech recognition, natural language processing, recommendation systems, and autonomous vehicles. As the amount\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-21\n",
        "#Using GPT-3.5-turbo model for information extraction\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You will be provided some text information and ask questions from the text provided\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Author-contribution statements and acknowledgements in research papers should state clearly and specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the preparation of their manuscript and analysis. They should also indicate which LLMs were used. This will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases, inaccuracies and improper source crediting. Likewise, scientific journals should be transparent about their use of LLMs, for example when selecting submitted manuscripts.Mention the large language model based product mentioned in the paragraph above:\"\n",
        "    }\n",
        "     ], temperature=0.8,top_p=0.1,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvC6KShviqAW",
        "outputId": "9bdd3f19-4be3-4a9d-b801-bfca7ea4228d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The large language model based product mentioned in the paragraph above is ChatGPT.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 22\n",
        "#Using completions API with the older versions of gpt models as they dont support chat completions. Chat completios is only available in gpt 3.5 and gpt4\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = openai.completions.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=\"Translate the following English text to French: '{Iam found of eating}'\",\n",
        ")\n",
        "text=response.choices[0].text\n",
        "print(text)"
      ],
      "metadata": {
        "id": "RJczswyZkUqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433bab81-eee1-459d-bc8e-f2b69dd2b911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Je suis fan de manger.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 23\n",
        "#Using completions API with the older versions of gpt models as they dont support chat completions. Chat completios is only available in gpt 3.5 and gpt4\n",
        "#Using parameters in completions method\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = openai.completions.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=\"Act as sudha murthy : '{Write a short story having only 500 words}'\",max_tokens=500,temperature=0.7,\n",
        ")\n",
        "text=response.choices[0].text\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23oqLHID07U8",
        "outputId": "da596f46-200d-48f8-cd35-3146f508451d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Once upon a time, there lived a young girl named Aditi. Aditi had a passion for books and so she was always seen carrying a book around with her.\n",
            "\n",
            "One day, Aditi was walking in the forest and she heard a faint voice calling out her name. It was a magical voice, and it sounded like it was coming from somewhere deep in the forest.\n",
            "\n",
            "Aditi followed the voice and soon she found herself in an open clearing. In the center of the clearing was a large tree with a large, shining book resting on a branch. Aditi slowly approached the book and gently opened it.\n",
            "\n",
            "Inside the book, Aditi found hundreds of stories, each one more amazing than the last. She began to read the stories and soon she was lost in a magical world.\n",
            "\n",
            "Aditi read for hours and hours, until she eventually fell asleep beneath the tree. When she awoke, she found the book gone. But Aditi was not worried, as she had already memorized all the stories and could recall them in her head.\n",
            "\n",
            "Aditi returned home with a newfound appreciation for reading and storytelling. She soon became a famous storyteller, traveling to different villages and sharing her stories with the people.\n",
            "\n",
            "Aditi's stories were filled with wisdom and joy, and soon, many more people began to visit her to listen to her stories. Aditi's stories were treasured by all and her legacy lives to this day. \n",
            "\n",
            "The magical book Aditi had found in the forest had opened up a new world of discovery for her. She had found a passion that would stay with her for the rest of her life.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 24\n",
        "#Using completions API with the older versions of gpt models as they dont support chat completions. Chat completios is only available in gpt 3.5 and gpt4\n",
        "#Using parameters in completions method by asking the model to give 2 different answers\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = openai.completions.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=\"Interview questions for the job of a data analyst'\",max_tokens=500,temperature=0.7,n=2\n",
        ")\n",
        "output1=response.choices[0].text\n",
        "print(output1)\n",
        "output2=response.choices[1].text\n",
        "print(output2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMkjALqx17iu",
        "outputId": "a9adeb47-1f79-488d-dce6-da1dec376502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. What experience do you have working with data analysis tools?\n",
            "2. What techniques do you use to accurately interpret data?\n",
            "3. How do you develop actionable insights from data?\n",
            "4. What methods do you use to ensure data accuracy and integrity?\n",
            "5. Describe a data analysis project you have completed from start to finish.\n",
            "6. What challenges have you faced when working with large datasets?\n",
            "7. How do you stay up-to-date on the latest trends in data analysis?\n",
            "8. How do you ensure that data is properly organized for analysis?\n",
            "9. What techniques do you use to identify patterns in data?\n",
            "10. How do you explore data to gain deeper insights?\n",
            "\n",
            "\n",
            "1. Describe your experience with data analysis and manipulation?\n",
            "2. What do you believe to be the most important skills for a data analyst?\n",
            "3. How do you stay up-to-date with the latest trends and techniques in data analysis?\n",
            "4. How do you ensure the accuracy and integrity of the data you analyze?\n",
            "5. What tools have you used for data analysis?\n",
            "6. What was the most challenging data analysis project you have worked on?\n",
            "7. How do you ensure data confidentiality and privacy?\n",
            "8. Describe the steps you would take to analyze a large dataset.\n",
            "9. What methods do you use to identify trends and patterns in data?\n",
            "10. How do you go about creating models to predict future outcomes?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-25\n",
        "#Text generation using chat completions\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "  ]\n",
        ")\n",
        "#To retrieve and Print the Generated Story\n",
        "#To retrieve and Print the response\n",
        "message=response.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "id": "b96kITu73qHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2ace65-f5e6-4e29-84fc-cc82ad7c87cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The World Series in 2020 was played in Arlington, Texas at the Globe Life Field.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-26\n",
        "#Text generation Enabling the json format and set the seed and count the number of tokens\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  seed=20,\n",
        "  response_format={ \"type\": \"json_object\" },\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2019?\"}\n",
        "  ]\n",
        ")\n",
        "#To retrieve and Print the Generated Story\n",
        "#To retrieve and Print the response\n",
        "message=response.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)\n",
        "#Total number of tokens\n",
        "total_tokens=response.usage.completion_tokens\n",
        "print(f\"Total tokens used are {total_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE89UGk3NN8h",
        "outputId": "7caa0fe4-da0b-4b03-eb18-07e1b2a2ce03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"winner\": \"Washington Nationals\"\n",
            "}\n",
            "Total tokens used are 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-1\n",
        "#Using GPT-4 model\n",
        "#Import openai library and initialize openai client\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-4-1106-preview\",\n",
        "  #model=\"gpt-4\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Do the binge search and find out the exact indian trade classification harmonised system (ITS HS) code matching with the latest dgcis hscode or heading and respond with only hscode and nothing else after that\"},\n",
        "    {\"role\": \"user\", \"content\": \"Indian dgcis Indian Trade Classification Harmonised System(ITC HS) code for Crop Name : Rice, Crop Type: Basmati Rice, Crop Scientific Names: Oryza sativa L.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"10063020\"},\n",
        "    {\"role\": \"user\", \"content\": \"Indian dgcis Indian Trade Classification Harmonised System(ITC HS) code for Crop Name : Rice, Crop Type: Non-Basmati Rice, Crop Scientific Names: Oryza sativa L.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"10063090\"},\n",
        "    # {\"role\": \"user\", \"content\": \"Indian dgcis Indian Trade Classification Harmonised System(ITC HS) code for Crop Name : Rice.\"},\n",
        "    # {\"role\": \"assistant\", \"content\": \"1006\"},\n",
        "    # {\"role\": \"user\", \"content\": \"Indian dgcis Indian Trade Classification Harmonised System(ITC HS) code or heading for Crop Name : Wheat.\"},\n",
        "    # {\"role\": \"assistant\", \"content\": \"1001\"},\n",
        "    # {\"role\": \"user\", \"content\": \"Indian dgcis Indian Trade Classification Harmonised System(ITC HS) code or heading for Crop Name : Barley\"},\n",
        "    # {\"role\": \"assistant\", \"content\": \"1003\"},\n",
        "    {\"role\": \"user\", \"content\": \"Latest indian dgcis Indian Trade Classification Harmonised System(ITC HS) code or Indian Trade Classification Harmonised System(ITC HS) heading for the grain Crop Name : Paddy, Crop type: non-Basmati, Crop Scientific Names: Oryza sativa L.\"}\n",
        "\n",
        "  ],temperature=0,top_p=0,\n",
        ")\n",
        "#To retrieve and Print\n",
        "message=completion.choices[0].message\n",
        "expanded_text=message.content.expandtabs(tabsize=4)\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "id": "Artqu4UhNcj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336ca611-d3ae-4e90-f204-5014d9fa6357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10061090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "17ajvjoAr-gj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQmKd/BitlscbDLaFu7442",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}