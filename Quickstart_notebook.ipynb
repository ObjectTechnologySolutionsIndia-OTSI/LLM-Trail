{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ObjectTechnologySolutionsIndia-OTSI/LLM-Trail/blob/Rajeshwari/Quickstart_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmE76hXMmnVK",
        "outputId": "4a1ac861-f8af-4dab-8c0c-5376f6a84f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.4.0-py3-none-any.whl (221 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m221.9/221.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.4.0\n"
          ]
        }
      ],
      "source": [
        "#Install OpenAI\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p461TDBSp888"
      },
      "outputs": [],
      "source": [
        "#To integrate OpenAI features into your project, include the following imports: import openai for accessing OpenAI functionality and import os for configuring the API key.\n",
        "import openai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uxg5KSA2qxN-"
      },
      "outputs": [],
      "source": [
        "#To access the value of the 'OPENAI_API_KEY' environment variable using the os module.\n",
        "os.environ['OPENAI_API_KEY'] = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iXmC-CjrGPu",
        "outputId": "f0993606-230e-4420-9b95-325b6acfce36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in the quaint village of Hubli, there lived a young girl named Anjali. Anjali was known for her kind heart and love for learning. Despite the financial struggles her family faced, Anjali dreamt of going to school and making a difference in the world.\n",
            "\n",
            "In those days, education for girls was not a priority, and the village lacked proper educational facilities. However, Anjali's determination was unwavering. She would often sit under a tree, absorbing knowledge from any book she could get her hands-on.\n",
            "\n",
            "One day, a travelling library arrived in the village. It was an initiative by a non-profit organization, led by a woman named Sudha. Sudha was inspired by the power of education and wanted to spread the joy of reading to young minds like Anjali.\n",
            "\n",
            "As Anjali explored the library's collection, she stumbled upon a book about a brave woman who fought for the rights of others. Inspired, Anjali decided to write a letter to Sudha, expressing her gratitude for bringing the library to their village and her dreams of becoming a change-maker like the woman in the book.\n",
            "\n",
            "Impressed with Anjali's determination, Sudha made a visit to the village. She was touched by the poverty and lack of opportunities that surrounded the young girl. Sudha knew she had to do something to help Anjali achieve her dreams.\n",
            "\n",
            "After speaking to Anjali's parents, Sudha offered to sponsor her education. Anjali's parents were hesitant, given their financial situation, but Sudha assured them that if Anjali studied hard, her efforts would be worth it.\n",
            "\n",
            "With Sudha's support, Anjali started attending school, excelling in her studies and participating in various extracurricular activities. She soon became a role model for other girls in the village, encouraging them to follow their dreams and never give up.\n",
            "\n",
            "Years passed, and Anjali's hard work paid off. She earned a scholarship to a prestigious university, where she continued to make a name for herself. She became a successful lawyer, fighting for the rights of the underprivileged and providing legal aid to those in need.\n",
            "\n",
            "Anjali never forgot the impact Sudha had on her life. She started her own foundation, dedicated to providing education and empowering young girls in rural areas. Sudha was touched when she heard about Anjali's work and offered her continuous support.\n",
            "\n",
            "Together, Sudha and Anjali transformed countless lives, breaking barriers and creating opportunities where there were none. Their tale of friendship, determination, and the power of education became an inspiration for people both near and far.\n",
            "\n",
            "The story of Sudha Murthy and Anjali serves as a reminder that with a little support, anyone can overcome adversity and make a positive change in the world.\n"
          ]
        }
      ],
      "source": [
        "#Example-1\n",
        "#Using GPT-3.5-turbo model, to generate a short story.\n",
        "#Import openai library and initialize openai client\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert story teller\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a short story assuming yourself as sudha murthy\"}\n",
        "  ],\n",
        ")\n",
        "#To retrieve and Print the Generated Story\n",
        "message=completion.choices[0].message\n",
        "expanded_text=message.content.expandtabs(tabsize=4)\n",
        "print(expanded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1rkhJ6AsFbg",
        "outputId": "4f8e19a7-71af-4019-8a68-79ffc2777284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She didn't go to the market.\n"
          ]
        }
      ],
      "source": [
        "#Example-2\n",
        "#Using GPT-3.5-turbo model to Convert ungrammatical statements into standard English.\n",
        "#Import openai library and initialize openai client\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with statements, and your task is to convert them to standard English.\"},\n",
        "    {\"role\": \"user\", \"content\": \"She no went to the market.\"}\n",
        "  ]\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text=message.content.expandtabs(tabsize=4)\n",
        "print(expanded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfe8zEc7z4Wq",
        "outputId": "537d8c1b-0a7c-41bb-837b-750dfadbb23a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jupiter is a really big planet in our Solar System. It is the fifth planet from the Sun and it is the largest planet. It is called a gas giant because it is made\n",
            "mostly of gas. Jupiter is not as big as the Sun, but it is much bigger than all the other planets put together.   Jupiter is very bright in the night sky and it is\n",
            "one of the brightest things we can see without a telescope. It has been known to people for a very long time, even before they started writing things down. It is\n",
            "named after a god from ancient Rome called Jupiter.   Sometimes, when we look at Jupiter from Earth, it is so bright that it can make shadows. It is usually the\n",
            "third-brightest thing we can see at night, after the Moon and Venus.\n"
          ]
        }
      ],
      "source": [
        "#Example-3\n",
        "#Using GPT-3.5-turbo model for text summarisation.\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Summarize content you are provided with for a second-grade student.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.\"}\n",
        "  ], max_tokens=500, temperature=0.7,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1-KTmZw0WXJ",
        "outputId": "dc026e20-b639-4e56-95d7-0607d650de85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content='Fruit,Color,Taste\\nNeoskizzles,Purple,Candy\\nLoheckles,Grayish Blue,Tart\\nPounits,Bright Green,Savory\\nLoopnovas,Neon Pink,Cotton Candy\\nGlow', role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ],
      "source": [
        "#Example-4\n",
        "#Using GPT-3.5-turbo model to parse unstructured data.\n",
        "#Import openai library and initialize openai client\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to parse it into CSV format.\"},\n",
        "    {\"role\": \"user\", \"content\": \"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of loopnovas which are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\"}\n",
        "  ], max_tokens=50, temperature=0.7,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "print(completion.choices[0].message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpFatbCW9A-t",
        "outputId": "acd4a254-a456-4169-b4b5-2f4d3536d0f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üòÇ\n"
          ]
        }
      ],
      "source": [
        "#Example-5\n",
        "#Using GPT-3.5-turbo model for emoji translation\n",
        "#Import openai library and initialize openai client\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with text, and your task is to translate it into emojis. Do not use any regular text. Do your best with emojis only.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Laugh out loud\"}\n",
        "  ], max_tokens=50, temperature=0.2,top_p=0.5\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sW88fRTn1hrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9071f1-752b-4894-efd2-3afe811e66d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This code defines a Log class that is used for logging events to a file. The class has an __init__ method that creates the necessary directories and opens the file\n",
            "in \"a+\" mode, allowing both reading and appending.   The log method takes an event as input, adds a unique event ID to it, and then writes the event as a JSON object\n",
            "to the file. Each event is written on a new line.  The state method reads the contents of the log file and returns a dictionary with two keys: \"complete\" and \"last\".\n",
            "The \"complete\" key contains a set of event IDs for events of type \"submit\" that were successful. The \"last\" key contains the last event that was read from the log\n",
            "file.\n"
          ]
        }
      ],
      "source": [
        "#Example-6\n",
        "#Using GPT-3.5-turbo model for Code explanation.\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with a piece of code, and your task is to explain it in a concise way.\"},\n",
        "    {\"role\": \"user\", \"content\": \"class Log:\\n def __init__(self, path):\\n dirname = os.path.dirname(path)\\n  os.makedirs(dirname, exist_ok=True)\\n f = open(path, \\\"a+\\\")\\n    \\n            # Check that the file is newline-terminated\\n            size = os.path.getsize(path)\\n            if size > 0:\\n                f.seek(size - 1)\\n                end = f.read(1)\\n                if end != \\\"\\\\n\\\":\\n                    f.write(\\\"\\\\n\\\")\\n            self.f = f\\n            self.path = path\\n    \\n        def log(self, event):\\n            event[\\\"_event_id\\\"] = str(uuid.uuid4())\\n            json.dump(event, self.f)\\n            self.f.write(\\\"\\\\n\\\")\\n    \\n        def state(self):\\n            state = {\\\"complete\\\": set(), \\\"last\\\": None}\\n            for line in open(self.path):\\n                event = json.loads(line)\\n                if event[\\\"type\\\"] == \\\"submit\\\" and event[\\\"success\\\"]:\\n                    state[\\\"complete\\\"].add(event[\\\"id\\\"])\\n                    state[\\\"last\\\"] = event\\n            return state\"\n",
        "    }\n",
        "     ], temperature=0.7,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-7\n",
        "#Using GPT-3.5-turbo model for bug fixation.\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with a piece of Python code, and your task is to find and fix bugs in it.\"},\n",
        "    {\"role\": \"user\", \"content\": \"import Random\\n    a = random.randint(1,12)\\n    b = random.randint(1,12)\\n    for i in range(10):\\n        question = \\\"What is \\\"+a+\\\" x \\\"+b+\\\"? \\\"\\n        answer = input(question)\\n        if answer = a*b\\n            print (Well done!)\\n        else:\\n            print(\\\"No.\\\")\"\n",
        "    }\n",
        "     ], temperature=0.7,top_p=0.7,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EB2iGUq6TL8",
        "outputId": "e4c930cd-1275-42cd-af50-f27034cab24a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import random  a = random.randint(1, 12) b = random.randint(1, 12)  for i in range(10):     question = \"What is \" + str(a) + \" x \" + str(b) + \"? \"     answer =\n",
            "int(input(question))     if answer == a * b:         print(\"Well done!\")     else:         print(\"No.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-8\n",
        "#Using GPT-3.5-turbo model for tweet classification.\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with a tweet, and your task is to classify its sentiment as positive, neutral, or negative.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I don't dislike pizza\"\n",
        "    }\n",
        "     ], temperature=0.7,top_p=0.7,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTp2-Mdi8aNr",
        "outputId": "ebb16846-0816-4d80-8b69-3d76ca9c20b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-9\n",
        "#Using GPT-3.5-turbo model for reasoning by using Chain of thought prompting\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert in anwering reasoning questions.\"},\n",
        "    {\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1\"},\n",
        "{\"role\": \"assistant\", \"content\": \"Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\"},\n",
        "{\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\"},\n",
        "{\"role\": \"assistant\", \"content\": \"Adding all the odd numbers (17, 19) gives 36. The answer is True.\"},\n",
        "{\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\"},\n",
        "{\"role\": \"assistant\", \"content\": \"Adding all the odd numbers (11, 13) gives 24. The answer is True.\"},\n",
        "{\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\"},\n",
        "{\"role\": \"assistant\", \"content\": \"Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\"},\n",
        "{\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\"},\n",
        "\n",
        "     ], temperature=0.7,top_p=0.7,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0g-31P3Bi1o",
        "outputId": "69e13709-c119-4b27-e183-6c3213266181"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-10\n",
        "#Using GPT-3.5-turbo model to generate interview questions\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Assume yourself as a datascience expert with over 20 years of working experience\"},\n",
        "        {\"role\": \"user\", \"content\": \"Create a list of 8 questions for an interview for the job of a data analyst\"},\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rxKB6G3FOrz",
        "outputId": "68e9c50c-2f97-433f-c27a-f5dae59fdbcf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Can you describe your experience in data analysis and how it aligns with the requirements of this role? 2. How do you approach data cleaning and preprocessing?\n",
            "Can you provide an example of a challenging data cleaning task you have encountered and how you resolved it? 3. What statistical techniques and tools do you use for\n",
            "data analysis? Can you explain how you have applied these techniques in previous projects? 4. How do you ensure the accuracy and reliability of your analysis\n",
            "results? Can you share any strategies or quality control measures you have implemented in your previous work? 5. Can you describe a time when you had to present\n",
            "complex data analysis findings to non-technical stakeholders? How did you ensure effective communication and understanding of the results? 6. How do you stay updated\n",
            "with the latest trends and advancements in data analysis? Can you provide an example of how you have applied a new technique or technology in your work? 7. Can you\n",
            "discuss a project where you had to work with large datasets? How did you handle the challenges associated with big data, such as data storage, processing, and\n",
            "analysis? 8. How do you approach problem-solving in data analysis? Can you share an example of a difficult problem you encountered and how you approached it to find\n",
            "a solution?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-11\n",
        "#Using GPT-3.5-turbo model for generating lesson plan writer for a specific topic\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Assume yourself as a datascience expert with writer skills\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a lesson plan for an induction to machine learning. The lesson plan should cover what,why is machine learning,benefits,steps involved,business understanding,preprocessing,machine learning models and evaluation metrics.\"},\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1jL_WfKGX4M",
        "outputId": "22025b42-2485-4552-94c3-6b4cdc156332"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lesson Plan: Introduction to Machine Learning  Objective: By the end of this lesson, students will have a clear understanding of what machine learning is, its\n",
            "benefits, the steps involved in the machine learning process, and the key concepts of business understanding, preprocessing, machine learning models, and evaluation\n",
            "metrics.  Duration: 60 minutes  Materials: - Presentation slides - Whiteboard or blackboard - Markers or chalk - Handouts (optional)  Lesson Outline:  I.\n",
            "Introduction (5 minutes)    - Welcome students and introduce the topic of machine learning.    - Explain the importance of machine learning in today's world.    -\n",
            "Share real-life examples of machine learning applications.  II. What is Machine Learning? (10 minutes)    - Define machine learning as the field of study that\n",
            "enables computers to learn and make predictions or decisions without being explicitly programmed.    - Discuss the difference between traditional programming and\n",
            "machine learning.    - Highlight the role of data in machine learning.  III. Benefits of Machine Learning (5 minutes)    - Explain the advantages of using machine\n",
            "learning, such as automation, scalability, and improved decision-making.    - Discuss how machine learning can help businesses gain insights from large amounts of\n",
            "data.  IV. Steps Involved in Machine Learning (10 minutes)    - Present an overview of the machine learning process, including the following steps:      1. Business\n",
            "Understanding: Define the problem and determine the goals of the machine learning project.      2. Data Preprocessing: Clean and prepare the data for analysis.\n",
            "3. Feature Engineering: Select and transform relevant features from the data.      4. Model Selection: Choose an appropriate machine learning model.      5. Model\n",
            "Training: Train the selected model using the prepared data.      6. Model Evaluation: Assess the performance of the trained model.      7. Model Deployment:\n",
            "Implement the model in a real-world scenario.  V. Business Understanding (10 minutes)    - Explain the importance of understanding the business context before\n",
            "starting a machine learning project.    - Discuss how to define the problem, identify the stakeholders, and set clear goals and objectives.  VI. Preprocessing (10\n",
            "minutes)    - Introduce the concept of data preprocessing and its significance in machine learning.    - Discuss common preprocessing techniques, such as handling\n",
            "missing values, dealing with outliers, and scaling features.  VII. Machine Learning Models (10 minutes)    - Provide an overview of popular machine learning models,\n",
            "such as linear regression, decision trees, and neural networks.    - Explain the strengths and weaknesses of different models and their suitability for different\n",
            "types of problems.  VIII. Evaluation Metrics (10 minutes)    - Discuss the importance of evaluation metrics in assessing the performance of machine learning models.\n",
            "- Present common evaluation metrics, such as accuracy, precision, recall, and F1 score.    - Explain how to choose the appropriate evaluation metric based on the\n",
            "problem at hand.  IX. Conclusion and Recap (5 minutes)    - Summarize the key points covered in the lesson.    - Encourage students to explore further resources and\n",
            "practice applying machine learning concepts.  X. Q&A and Discussion (5 minutes)    - Allow students to ask questions and engage in a discussion about the lesson\n",
            "content.  Note: The duration of each section can be adjusted based on the pace of the class and the level of prior knowledge. Additionally, interactive activities,\n",
            "case studies, or hands-on exercises can be incorporated to enhance student engagement and understanding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-12\n",
        "#Using GPT-3.5-turbo model for pros and cons discusser\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "       {\"role\": \"user\", \"content\": \"Analyze the pros and cons of remote work vs. office work\"},\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luY5LmHDMtoi",
        "outputId": "f16cb0e9-d1e2-4028-86bb-7ede8a13c6cf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pros of remote work:  1. Flexibility: Remote work allows individuals to have more control over their work schedule. They can choose when and where they work, which\n",
            "can lead to better work-life balance and increased productivity.  2. Cost savings: Remote work eliminates commuting costs, such as transportation expenses and\n",
            "parking fees. It also reduces expenses related to office attire, meals, and snacks, leading to potential financial savings.  3. Increased job opportunities: Remote\n",
            "work opens up job opportunities for individuals who may not be able to relocate or commute to a traditional office. It allows companies to hire talent from different\n",
            "geographical locations, leading to a more diverse workforce.  4. Reduced distractions: Working remotely can provide a quieter and more focused environment, free from\n",
            "office distractions like interruptions from colleagues or unnecessary meetings. This can lead to increased concentration and productivity.  5. Environmental\n",
            "benefits: Remote work reduces the need for commuting, resulting in lower carbon emissions and less traffic congestion. It also reduces the use of office resources,\n",
            "such as electricity and water.  Cons of remote work:  1. Lack of social interaction: Remote work can be isolating, as it eliminates face-to-face interactions with\n",
            "colleagues. This can lead to feelings of loneliness and decreased team collaboration, which may impact creativity and innovation.  2. Communication challenges:\n",
            "Remote work relies heavily on technology for communication, which can sometimes lead to misinterpretation or delays in receiving important information. Building\n",
            "rapport and trust with colleagues may also be more challenging without regular in-person interactions.  3. Blurred work-life boundaries: Working from home can make\n",
            "it difficult to separate work and personal life, leading to longer working hours and potential burnout. It may also be challenging to create a dedicated workspace,\n",
            "especially for individuals with limited living space.  4. Limited career growth opportunities: Remote work may limit opportunities for career advancement, as it can\n",
            "be harder to build relationships and network with colleagues and superiors. Being physically present in the office often provides more visibility and chances for\n",
            "professional development.  5. Dependence on technology: Remote work heavily relies on stable internet connections and reliable technology. Technical issues or\n",
            "internet outages can disrupt productivity and cause frustration.  Pros of office work:  1. Face-to-face collaboration: Working in an office allows for direct and\n",
            "immediate collaboration with colleagues. This can foster creativity, problem-solving, and teamwork, as individuals can easily bounce ideas off each other and engage\n",
            "in spontaneous discussions.  2. Networking opportunities: Being physically present in an office environment provides more opportunities for networking and building\n",
            "relationships with colleagues, superiors, and clients. This can lead to career growth and advancement.  3. Separation of work and personal life: Having a dedicated\n",
            "office space allows individuals to create a clear separation between work and personal life. Leaving the office at the end of the day can help establish a healthy\n",
            "work-life balance.  4. Access to resources and equipment: Offices are typically equipped with necessary resources, such as high-speed internet, specialized software,\n",
            "and office supplies. This can enhance productivity and efficiency, especially for roles that require specific tools or equipment.  5. Social interaction and\n",
            "camaraderie: Office work allows for social interactions, fostering a sense of camaraderie and team spirit. Regular face-to-face interactions can improve\n",
            "communication, build trust, and create a positive work environment.  Cons of office work:  1. Commuting and associated costs: Office work often requires commuting,\n",
            "which can be time-consuming and expensive. Commuting can also contribute to stress, fatigue, and environmental pollution.  2. Distractions and interruptions: Office\n",
            "environments can be noisy and filled with distractions, such as chatty colleagues, impromptu meetings, or office politics. These interruptions can hinder\n",
            "concentration and productivity.  3. Lack of flexibility: Office work typically follows a fixed schedule, limiting flexibility for individuals who may have personal\n",
            "commitments or prefer alternative work hours. This lack of flexibility can impact work-life balance and employee satisfaction.  4. Higher expenses: Office work comes\n",
            "with additional expenses, such as transportation costs, office attire, meals, and snacks. These expenses can add up and impact personal finances.  5. Limited\n",
            "geographical reach: Office work often requires employees to be physically present at a specific location, limiting job opportunities for individuals who cannot\n",
            "relocate or commute to a particular office. This can result in a less diverse workforce.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-13\n",
        "#Using GPT-3.5-turbo model for review classifier\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You will be presented with user reviews and your job is to provide a set of tags from the following list. Provide your answer in bullet point form. Choose ONLY from the list of tags provided here (choose either the positive or the negative tag but NOT both):\\n    \\n    - Provides good value for the price OR Costs too much\\n    - Works better than expected OR Did not work as well as expected\\n    - Includes essential features OR Lacks essential features\\n    - Easy to use OR Difficult to use\\n    - High quality and durability OR Poor quality and durability\\n    - Easy and affordable to maintain or repair OR Difficult or costly to maintain or repair\\n    - Easy to transport OR Difficult to transport\\n    - Easy to store OR Difficult to store\\n    - Compatible with other devices or systems OR Not compatible with other devices or systems\\n    - Safe and user-friendly OR Unsafe or hazardous to use\\n    - Excellent customer support OR Poor customer support\\n    - Generous and comprehensive warranty OR Limited or insufficient warranty\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"I recently purchased the Inflatotron 2000 airbed for a camping trip and wanted to share my experience with others. Overall, I found the airbed to be a mixed bag with some positives and negatives.\\n    \\n    Starting with the positives, the Inflatotron 2000 is incredibly easy to set up and inflate. It comes with a built-in electric pump that quickly inflates the bed within a few minutes, which is a huge plus for anyone who wants to avoid the hassle of manually pumping up their airbed. The bed is also quite comfortable to sleep on and offers decent support for your back, which is a major plus if you have any issues with back pain.\\n    \\n    On the other hand, I did experience some negatives with the Inflatotron 2000. Firstly, I found that the airbed is not very durable and punctures easily. During my camping trip, the bed got punctured by a stray twig that had fallen on it, which was quite frustrating. Secondly, I noticed that the airbed tends to lose air overnight, which meant that I had to constantly re-inflate it every morning. This was a bit annoying as it disrupted my sleep and made me feel less rested in the morning.\\n    \\n    Another negative point is that the Inflatotron 2000 is quite heavy and bulky, which makes it difficult to transport and store. If you're planning on using this airbed for camping or other outdoor activities, you'll need to have a large enough vehicle to transport it and a decent amount of storage space to store it when not in use.\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLyX3X8iNxaZ",
        "outputId": "cef30103-04ae-4f00-a6c5-42d3ab567113"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Easy to use - High quality and durability - Difficult to transport - Difficult to store\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-14\n",
        "#Using GPT-3.5-turbo model for Meeting notes summarizer\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "   {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You will be provided with meeting notes, and your task is to summarize the meeting as follows:\\n    \\n    -Overall summary of discussion\\n    -Action items (what needs to be done and who is doing it)\\n    -If applicable, a list of topics that need to be discussed more fully in the next meeting.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Meeting Date: March 5th, 2050\\n    Meeting Time: 2:00 PM\\n    Location: Conference Room 3B, Intergalactic Headquarters\\n    \\n    Attendees:\\n    - Captain Stardust\\n    - Dr. Quasar\\n    - Lady Nebula\\n    - Sir Supernova\\n    - Ms. Comet\\n    \\n    Meeting called to order by Captain Stardust at 2:05 PM\\n    \\n    1. Introductions and welcome to our newest team member, Ms. Comet\\n    \\n    2. Discussion of our recent mission to Planet Zog\\n    - Captain Stardust: \\\"Overall, a success, but communication with the Zogians was difficult. We need to improve our language skills.\\\"\\n    - Dr. Quasar: \\\"Agreed. I'll start working on a Zogian-English dictionary right away.\\\"\\n    - Lady Nebula: \\\"The Zogian food was out of this world, literally! We should consider having a Zogian food night on the ship.\\\"\\n    \\n    3. Addressing the space pirate issue in Sector 7\\n    - Sir Supernova: \\\"We need a better strategy for dealing with these pirates. They've already plundered three cargo ships this month.\\\"\\n    - Captain Stardust: \\\"I'll speak with Admiral Starbeam about increasing patrols in that area.\\n    - Dr. Quasar: \\\"I've been working on a new cloaking technology that could help our ships avoid detection by the pirates. I'll need a few more weeks to finalize the prototype.\\\"\\n    \\n    4. Review of the annual Intergalactic Bake-Off\\n    - Lady Nebula: \\\"I'm happy to report that our team placed second in the competition! Our Martian Mud Pie was a big hit!\\\"\\n    - Ms. Comet: \\\"Let's aim for first place next year. I have a secret recipe for Jupiter Jello that I think could be a winner.\\\"\\n    \\n    5. Planning for the upcoming charity fundraiser\\n    - Captain Stardust: \\\"We need some creative ideas for our booth at the Intergalactic Charity Bazaar.\\\"\\n    - Sir Supernova: \\\"How about a 'Dunk the Alien' game? We can have people throw water balloons at a volunteer dressed as an alien.\\\"\\n    - Dr. Quasar: \\\"I can set up a 'Name That Star' trivia game with prizes for the winners.\\\"\\n    - Lady Nebula: \\\"Great ideas, everyone. Let's start gathering the supplies and preparing the games.\\\"\\n    \\n    6. Upcoming team-building retreat\\n    - Ms. Comet: \\\"I would like to propose a team-building retreat to the Moon Resort and Spa. It's a great opportunity to bond and relax after our recent missions.\\\"\\n    - Captain Stardust: \\\"Sounds like a fantastic idea. I'll check the budget and see if we can make it happen.\\\"\\n    \\n    7. Next meeting agenda items\\n    - Update on the Zogian-English dictionary (Dr. Quasar)\\n    - Progress report on the cloaking technology (Dr. Quasar)\\n    - Results of increased patrols in Sector 7 (Captain Stardust)\\n    - Final preparations for the Intergalactic Charity Bazaar (All)\\n    \\n    Meeting adjourned at 3:15 PM. Next meeting scheduled for March 19th, 2050 at 2:00 PM in Conference Room 3B, Intergalactic Headquarters.\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7,frequency_penalty=0.5\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9fQkGqiQ5zz",
        "outputId": "74fec4b2-323c-4e85-c96d-4efc7980b90f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Summary of Discussion: The meeting began with introductions and a warm welcome to the newest team member, Ms. Comet. The team then discussed their recent\n",
            "mission to Planet Zog, acknowledging its success but also highlighting the need for improved communication skills. Dr. Quasar volunteered to work on a Zogian-English\n",
            "dictionary. The team also discussed the issue of space pirates in Sector 7 and agreed on the need for a better strategy. Captain Stardust planned to speak with\n",
            "Admiral Starbeam about increasing patrols, while Dr. Quasar worked on a cloaking technology prototype to avoid detection by pirates. The team celebrated their\n",
            "second-place finish in the annual Intergalactic Bake-Off and made plans to aim for first place next year with Ms. Comet's secret recipe. They also discussed ideas\n",
            "for their booth at an upcoming charity fundraiser, including a \"Dunk the Alien\" game and a \"Name That Star\" trivia game. Finally, they considered a team-building\n",
            "retreat to the Moon Resort and Spa.  Action Items: 1. Dr. Quasar will work on creating a Zogian-English dictionary. 2. Captain Stardust will speak with Admiral\n",
            "Starbeam about increasing patrols in Sector 7. 3. Dr. Quasar will finalize the prototype for cloaking technology. 4. All team members will gather supplies and\n",
            "prepare games for the Intergalactic Charity Bazaar.  Topics for Next Meeting: 1. Update on the progress of the Zogian-English dictionary (Dr. Quasar) 2. Progress\n",
            "report on the cloaking technology prototype (Dr. Quasar) 3. Results of increased patrols in Sector 7 (Captain Stardust) 4. Final preparations for the Intergalactic\n",
            "Charity Bazaar (All)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-15\n",
        "#Using GPT-3.5-turbo model for Natural Language to SQL\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "   {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"Given the following SQL tables, your job is to write queries given a user‚Äôs request.\\n    \\n    CREATE TABLE Orders (\\n      OrderID int,\\n      CustomerID int,\\n      OrderDate datetime,\\n      OrderTime varchar(8),\\n      PRIMARY KEY (OrderID)\\n    );\\n    \\n    CREATE TABLE OrderDetails (\\n      OrderDetailID int,\\n      OrderID int,\\n      ProductID int,\\n      Quantity int,\\n      PRIMARY KEY (OrderDetailID)\\n    );\\n    \\n    CREATE TABLE Products (\\n      ProductID int,\\n      ProductName varchar(50),\\n      Category varchar(50),\\n      UnitPrice decimal(10, 2),\\n      Stock int,\\n      PRIMARY KEY (ProductID)\\n    );\\n    \\n    CREATE TABLE Customers (\\n      CustomerID int,\\n      FirstName varchar(50),\\n      LastName varchar(50),\\n      Email varchar(100),\\n      Phone varchar(20),\\n      PRIMARY KEY (CustomerID)\\n    );\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Write a SQL query which computes the average total order value for all orders on 2023-04-01.\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW2WzFw1RU2_",
        "outputId": "e0ef45e8-0f2a-4da5-f611-bf4a0f50a5b1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT AVG(total_order_value) AS average_order_value FROM (     SELECT SUM(od.Quantity * p.UnitPrice) AS total_order_value     FROM Orders o     JOIN OrderDetails od\n",
            "ON o.OrderID = od.OrderID     JOIN Products p ON od.ProductID = p.ProductID     WHERE o.OrderDate = '2023-04-01'     GROUP BY o.OrderID ) AS subquery;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-16\n",
        "#Using GPT-3.5-turbo model for Language translation\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "   {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You will be provided with a sentence in English, and your task is to translate it into German.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Languaage translation is the most booming topic in current situation\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAgpdSNlR78F",
        "outputId": "c3219707-6942-4a7a-a8f0-198a14887bb8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sprach√ºbersetzung ist das derzeit am st√§rksten wachsende Thema.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-17\n",
        "#Using GPT-3.5-turbo model for improving code efficiency\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "   {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You will be provided with a piece of Python code, and your task is to provide ideas for efficiency improvements.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"from typing import List\\n def has_sum_k(nums: List[int], k: int) -> bool:\\n        \\\"\\\"\\\"\\n        Returns True if there are two distinct elements in nums such that their sum \\n        is equal to k, and otherwise returns False.\\n        \\\"\\\"\\\"\\n        n = len(nums)\\n        for i in range(n):\\n            for j in range(i+1, n):\\n                if nums[i] + nums[j] == k:\\n                    return True\\n        return False\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnl-Lt6sTUCW",
        "outputId": "b3009ebf-27c2-416d-add4-bab323cd9063"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One possible improvement is to use a set to store the elements of `nums` as we iterate through the list. This way, we can quickly check if the complement of the\n",
            "current element exists in the set, which would indicate that we have found two distinct elements that sum to `k`. This approach has a time complexity of O(n), where\n",
            "n is the length of `nums`.  Here's an updated version of the code that implements this improvement:  ```python from typing import List  def has_sum_k(nums:\n",
            "List[int], k: int) -> bool:     \"\"\"     Returns True if there are two distinct elements in nums such that their sum      is equal to k, and otherwise returns False.\n",
            "\"\"\"     num_set = set()     for num in nums:         complement = k - num         if complement in num_set:             return True         num_set.add(num)\n",
            "return False ```  Another possible improvement is to sort the `nums` list before iterating through it. This way, we can use a two-pointer approach where we maintain\n",
            "two pointers, one at the beginning of the list and one at the end. We compare the sum of the elements at the two pointers with `k` and adjust the pointers\n",
            "accordingly. This approach has a time complexity of O(n log n) due to the sorting step.  Here's an updated version of the code that implements this improvement:\n",
            "```python from typing import List  def has_sum_k(nums: List[int], k: int) -> bool:     \"\"\"     Returns True if there are two distinct elements in nums such that\n",
            "their sum      is equal to k, and otherwise returns False.     \"\"\"     nums.sort()     left = 0     right = len(nums) - 1     while left < right:         current_sum\n",
            "= nums[left] + nums[right]         if current_sum == k:             return True         elif current_sum < k:             left += 1         else:             right\n",
            "-= 1     return False ```  Note that the second approach assumes that the input list `nums` can be modified. If modifying the input list is not allowed, you can\n",
            "create a copy of the list and sort the copy instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-18\n",
        "#Using GPT-3.5-turbo model for writing a function from the specification\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"write a python code which takes 3 numbers as input and finds the greatest of them \"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e5yw7fmU6jc",
        "outputId": "05051a38-50b0-4a13-8d7f-3c267f88e22f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a Python code that takes 3 numbers as input and finds the greatest of them:\n",
            "\n",
            "```python\n",
            "# Take input from the user\n",
            "num1 = float(input(\"Enter the first number: \"))\n",
            "num2 = float(input(\"Enter the second number: \"))\n",
            "num3 = float(input(\"Enter the third number: \"))\n",
            "\n",
            "# Compare the numbers to find the greatest\n",
            "if num1 >= num2 and num1 >= num3:\n",
            "    greatest = num1\n",
            "elif num2 >= num1 and num2 >= num3:\n",
            "    greatest = num2\n",
            "else:\n",
            "    greatest = num3\n",
            "\n",
            "# Print the greatest number\n",
            "print(\"The greatest number is:\", greatest)\n",
            "```\n",
            "\n",
            "In this code, we take three numbers as input from the user using the `input()` function. We then compare the numbers using if-elif-else statements to find the greatest number. Finally, we print the greatest number using the `print()` function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-19\n",
        "#Using GPT-3.5-turbo model for text completion\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"Complete the text that will be provided to you\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Machine learning is a\"\n",
        "    }\n",
        "     ], temperature=0.8,top_p=0.1,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYb_wIaWV2mi",
        "outputId": "81d1166d-4456-440b-be48-3210767f9b61"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subfield of artificial intelligence that focuses on the development of algorithms and models that allow computers to learn and make predictions or decisions without being explicitly programmed. It involves the use of statistical techniques and data analysis to enable machines to learn from and adapt to new information. Machine learning algorithms can be trained on large datasets to recognize patterns, make predictions, or solve complex problems. This technology has applications in various fields, including image and speech recognition, natural language processing, recommendation systems, and autonomous vehicles. As the amount\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-20\n",
        "#Using GPT-3.5-turbo model for information extraction\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You will be provided some text information and ask questions from the text provided\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Author-contribution statements and acknowledgements in research papers should state clearly and specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the preparation of their manuscript and analysis. They should also indicate which LLMs were used. This will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases, inaccuracies and improper source crediting. Likewise, scientific journals should be transparent about their use of LLMs, for example when selecting submitted manuscripts.Mention the large language model based product mentioned in the paragraph above:\"\n",
        "    }\n",
        "     ], temperature=0.8,top_p=0.1,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvC6KShviqAW",
        "outputId": "3aa2d47a-a6a9-4a2b-8d98-14a471fcaf4a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The large language model based product mentioned in the paragraph above is ChatGPT.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJczswyZkUqY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOgo0UktQKifOedC2e5w/j",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}