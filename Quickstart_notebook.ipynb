{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ObjectTechnologySolutionsIndia-OTSI/LLM-Trail/blob/Rajeshwari/Quickstart_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmE76hXMmnVK",
        "outputId": "6418474a-585e-443f-972b-9d0e7d6b73ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.6.0-py3-none-any.whl (225 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/225.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/225.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.6.0 typing-extensions-4.9.0\n"
          ]
        }
      ],
      "source": [
        "#Install OpenAI\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p461TDBSp888"
      },
      "outputs": [],
      "source": [
        "#To integrate OpenAI features into your project, include the following imports: import openai for accessing OpenAI functionality and import os for configuring the API key.\n",
        "import openai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uxg5KSA2qxN-"
      },
      "outputs": [],
      "source": [
        "#To access the value of the 'OPENAI_API_KEY' environment variable using the os module.\n",
        "os.environ['OPENAI_API_KEY'] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iXmC-CjrGPu",
        "outputId": "6f9a6707-203e-409c-de4f-a661d264fe83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in a small village nestled amidst rolling hills, there lived a young girl named Lila. She had big dreams and an even bigger heart. Lila was passionate about education and believed it to be the key to unlocking a brighter future for her community.\n",
            "\n",
            "However, in a world where education was considered a luxury, Lila's dreams seemed far-fetched. Her parents, like many others in the village, worked hard just to make ends meet. Despite their limitations, Lila's parents recognized their daughter's potential and encouraged her to pursue her aspirations, even if it meant sacrificing their own desires.\n",
            "\n",
            "With unwavering determination, Lila embarked on a mission to bring education to her fellow villagers. She would gather children beneath the large banyan tree in the center of the village and teach them everything she knew. Lila believed that knowledge should be shared and that everyone, regardless of their circumstances, deserved an opportunity to learn.\n",
            "\n",
            "News of Lila's teaching efforts reached the ears of the village elders, who were initially skeptical. They dismissed the idea, deeming it nothing more than a daydream of an innocent girl. However, Lila's earnestness and sincerity touched the hearts of a few kind souls.\n",
            "\n",
            "One of those souls was Sudha, a wise and compassionate woman who had dedicated her life to uplift the underprivileged. She had been following Lila's efforts from afar, impressed by the young girl's determination and courage. Sudha decided to pay a visit to the village to witness Lila's teaching firsthand.\n",
            "\n",
            "Upon arriving, Sudha was greeted with a warm smile by Lila. As she observed Lila teaching under the banyan tree, she realized the impact this young girl had made on the lives of the village children. With the spark of education ignited within them, they were filled with hope and a thirst for knowledge.\n",
            "\n",
            "Deeply moved, Sudha decided to support Lila's cause. She organized a campaign to gather books, school supplies, and financial donations from the surrounding towns and cities. The response was overwhelming as people came forward to contribute, inspired by Lila's story and her determination.\n",
            "\n",
            "With Sudha's help, a small school was built in the village. Lila's dream had finally come true. The children who once toiled in the fields were now sitting behind desks, learning and growing. They were eager to make the most of this opportunity and show the world what they were capable of.\n",
            "\n",
            "Years passed, and Lila continued her work, expanding her efforts to neighboring villages as well. She remained a beacon of hope, reminding everyone that even the smallest act of kindness could have a profound impact on the lives of others. Lila's passion for education touched countless lives and became a catalyst for change in the entire region.\n",
            "\n",
            "Sudha and Lila became inseparable partners, dedicating their lives to empowering the underprivileged through education. Together, they transformed countless lives, proving that dreams, when combined with determination and selflessness, have the power to transcend all barriers.\n",
            "\n",
            "And so, the story of Lila and Sudha became an inspiration for generations to come, reminding people that each of us has the ability to make a difference in the world, no matter how small or large our contributions may be.\n"
          ]
        }
      ],
      "source": [
        "#Example-1\n",
        "#Using GPT-3.5-turbo model, to generate a short story.\n",
        "#Import openai library and initialize openai client\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert story teller\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a short story assuming yourself as sudha murthy\"}\n",
        "  ],\n",
        ")\n",
        "#To retrieve and Print the Generated Story\n",
        "message=completion.choices[0].message\n",
        "expanded_text=message.content.expandtabs(tabsize=4)\n",
        "print(expanded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1rkhJ6AsFbg",
        "outputId": "31885b9b-893d-4970-daaf-3887c1d578fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She didn't go to the market.\n"
          ]
        }
      ],
      "source": [
        "#Example-2\n",
        "#Using GPT-3.5-turbo model to Convert ungrammatical statements into standard English.\n",
        "#Import openai library and initialize openai client\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with statements, and your task is to convert them to standard English.\"},\n",
        "    {\"role\": \"user\", \"content\": \"She no went to the market.\"}\n",
        "  ]\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text=message.content.expandtabs(tabsize=4)\n",
        "print(expanded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfe8zEc7z4Wq",
        "outputId": "92473220-f1f2-43d9-b832-5683830ec244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jupiter is a big planet in our Solar System. It is the fifth planet from the Sun and the largest one. It is made mostly of gas. Jupiter is really bright and can be\n",
            "seen in the sky at night. People have known about Jupiter for a very long time. It is named after a god from ancient Rome. Sometimes, Jupiter is so bright that it\n",
            "can make shadows on Earth. It is usually the third-brightest thing we can see in the sky at night, after the Moon and Venus.\n"
          ]
        }
      ],
      "source": [
        "#Example-3\n",
        "#Using GPT-3.5-turbo model for text summarisation.\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Summarize content you are provided with for a second-grade student.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.\"}\n",
        "  ], max_tokens=500, temperature=0.7,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpFatbCW9A-t",
        "outputId": "a342f571-e639-4b87-f8ee-ce1ecdc9f33a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "😂\n"
          ]
        }
      ],
      "source": [
        "#Example-4\n",
        "#Using GPT-3.5-turbo model for emoji translation\n",
        "#Import openai library and initialize openai client\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with text, and your task is to translate it into emojis. Do not use any regular text. Do your best with emojis only.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Laugh out loud\"}\n",
        "  ], max_tokens=50, temperature=0.2,top_p=0.5\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sW88fRTn1hrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9da740-0a9e-4f82-b19f-7ca45f9128ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This code defines a Log class that is used to log events to a file. The class has three methods:  1. __init__(self, path): This method is the constructor for the Log\n",
            "class. It takes a path argument, which is the path to the log file. It creates the directory for the log file if it doesn't exist, and opens the file in \"a+\" mode\n",
            "(append and read). It also checks if the file is newline-terminated and adds a newline if it's not. It assigns the file object and the path to instance variables.\n",
            "2. log(self, event): This method is used to log an event. It takes an event argument, which is a dictionary representing the event. It adds a unique ID to the event,\n",
            "serializes the event to JSON format, and writes it to the log file. It also adds a newline after writing the event.  3. state(self): This method returns the state of\n",
            "the log. It reads each line from the log file, parses it as JSON, and checks if the event type is \"submit\" and the event was successful. It keeps track of the set of\n",
            "completed event IDs and the last event in the state. It returns a dictionary with the \"complete\" set and the \"last\" event.\n"
          ]
        }
      ],
      "source": [
        "#Example-5\n",
        "#Using GPT-3.5-turbo model for Code explanation.\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with a piece of code, and your task is to explain it in a concise way.\"},\n",
        "    {\"role\": \"user\", \"content\": \"class Log:\\n def __init__(self, path):\\n dirname = os.path.dirname(path)\\n  os.makedirs(dirname, exist_ok=True)\\n f = open(path, \\\"a+\\\")\\n    \\n            # Check that the file is newline-terminated\\n            size = os.path.getsize(path)\\n            if size > 0:\\n                f.seek(size - 1)\\n                end = f.read(1)\\n                if end != \\\"\\\\n\\\":\\n                    f.write(\\\"\\\\n\\\")\\n            self.f = f\\n            self.path = path\\n    \\n        def log(self, event):\\n            event[\\\"_event_id\\\"] = str(uuid.uuid4())\\n            json.dump(event, self.f)\\n            self.f.write(\\\"\\\\n\\\")\\n    \\n        def state(self):\\n            state = {\\\"complete\\\": set(), \\\"last\\\": None}\\n            for line in open(self.path):\\n                event = json.loads(line)\\n                if event[\\\"type\\\"] == \\\"submit\\\" and event[\\\"success\\\"]:\\n                    state[\\\"complete\\\"].add(event[\\\"id\\\"])\\n                    state[\\\"last\\\"] = event\\n            return state\"\n",
        "    }\n",
        "     ], temperature=0.7,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-6\n",
        "#Using GPT-3.5-turbo model for bug fixation.\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with a piece of Python code, and your task is to find and fix bugs in it.\"},\n",
        "    {\"role\": \"user\", \"content\": \"import Random\\n    a = random.randint(1,12)\\n    b = random.randint(1,12)\\n    for i in range(10):\\n        question = \\\"What is \\\"+a+\\\" x \\\"+b+\\\"? \\\"\\n        answer = input(question)\\n        if answer = a*b\\n            print (Well done!)\\n        else:\\n            print(\\\"No.\\\")\"\n",
        "    }\n",
        "     ], temperature=0.7,top_p=0.7,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EB2iGUq6TL8",
        "outputId": "325f1f06-d990-4974-9a0d-fde8065659eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import random  a = random.randint(1, 12) b = random.randint(1, 12)  for i in range(10):     question = \"What is \" + str(a) + \" x \" + str(b) + \"? \"     answer =\n",
            "int(input(question))          if answer == a * b:         print(\"Well done!\")     else:         print(\"No.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-7\n",
        "#Using GPT-3.5-turbo model for tweet classification.\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You will be provided with a tweet, and your task is to classify its sentiment as positive, neutral, or negative.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I don't dislike pizza\"\n",
        "    }\n",
        "     ], temperature=0.7,top_p=0.7,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTp2-Mdi8aNr",
        "outputId": "55005d19-49ec-4a81-9332-02ce05858ddc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-10\n",
        "#Using GPT-3.5-turbo model for reasoning by using Chain of thought prompting\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert in anwering reasoning questions.\"},\n",
        "    {\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1\"},\n",
        "{\"role\": \"assistant\", \"content\": \"Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\"},\n",
        "{\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\"},\n",
        "{\"role\": \"assistant\", \"content\": \"Adding all the odd numbers (17, 19) gives 36. The answer is True.\"},\n",
        "{\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\"},\n",
        "{\"role\": \"assistant\", \"content\": \"Adding all the odd numbers (11, 13) gives 24. The answer is True.\"},\n",
        "{\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\"},\n",
        "{\"role\": \"assistant\", \"content\": \"Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\"},\n",
        "{\"role\": \"user\", \"content\": \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\"},\n",
        "\n",
        "     ], temperature=0.7,top_p=0.7,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0g-31P3Bi1o",
        "outputId": "aeddcce1-ab63-4990-cd16-1956c4fa676f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-11\n",
        "#Using GPT-3.5-turbo model to generate interview questions\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Assume yourself as a datascience expert with over 20 years of working experience\"},\n",
        "        {\"role\": \"user\", \"content\": \"Create a list of 8 questions for an interview for the job of a data analyst\"},\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rxKB6G3FOrz",
        "outputId": "bf9601de-70a0-421b-a1da-b678f68effac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Can you describe your experience in data analysis and how it aligns with the requirements of this role? 2. How do you approach data cleaning and preprocessing\n",
            "tasks? Can you provide an example of a challenging data cleaning problem you encountered and how you resolved it? 3. What statistical techniques and tools do you\n",
            "typically use for data analysis? Can you give an example of a project where you applied these techniques to derive meaningful insights? 4. How do you ensure the\n",
            "accuracy and reliability of your analysis results? Can you share any strategies or practices you follow to validate your findings? 5. Can you explain your experience\n",
            "with data visualization? How do you choose the most appropriate visualization techniques to effectively communicate insights to stakeholders? 6. Have you worked with\n",
            "large datasets before? How do you handle scalability and performance issues when working with big data? 7. Can you describe a situation where you had to work with a\n",
            "cross-functional team or collaborate with other departments to gather data or achieve a common goal? How did you ensure effective communication and collaboration? 8.\n",
            "How do you stay updated with the latest trends and advancements in the field of data analysis? Can you provide an example of how you applied a new technique or\n",
            "technology to improve your analysis process?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-12\n",
        "#Using GPT-3.5-turbo model for generating lesson plan writer for a specific topic\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Assume yourself as a datascience expert with writer skills\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a lesson plan for an induction to machine learning. The lesson plan should cover what,why is machine learning,benefits,steps involved,business understanding,preprocessing,machine learning models and evaluation metrics.\"},\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1jL_WfKGX4M",
        "outputId": "cf3c0141-1e02-4b62-e283-9f069000d9ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lesson Plan: Introduction to Machine Learning  Objective: By the end of this lesson, students will have a clear understanding of what machine learning is, its\n",
            "benefits, the steps involved in the machine learning process, and the key concepts of business understanding, preprocessing, machine learning models, and evaluation\n",
            "metrics.  Duration: 60 minutes  Materials: - Presentation slides - Whiteboard or flipchart - Markers  Lesson Outline:  1. Introduction (5 minutes)    a. Greet the\n",
            "students and introduce yourself as a data science expert.    b. Explain the importance of machine learning in today's world and its applications in various\n",
            "industries.  2. What is Machine Learning? (10 minutes)    a. Define machine learning as a subset of artificial intelligence that enables computers to learn and make\n",
            "predictions or decisions without being explicitly programmed.    b. Discuss the difference between traditional programming and machine learning.    c. Highlight the\n",
            "three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.  3. Why Machine Learning? (10 minutes)    a. Discuss\n",
            "the benefits of machine learning, such as automation, improved decision-making, pattern recognition, and scalability.    b. Provide real-life examples of how machine\n",
            "learning is being used in different industries, such as healthcare, finance, and marketing.  4. Steps Involved in Machine Learning (15 minutes)    a. Explain the\n",
            "general steps involved in the machine learning process:       - Business Understanding: Define the problem and understand the business objectives.       - Data\n",
            "Preprocessing: Clean and prepare the data for analysis.       - Model Building: Select and train a machine learning model.       - Model Evaluation: Assess the\n",
            "model's performance using evaluation metrics.       - Model Deployment: Implement the model in a real-world scenario.    b. Discuss the iterative nature of the\n",
            "process and the importance of continuous improvement.  5. Business Understanding (10 minutes)    a. Explain the significance of understanding the business problem\n",
            "before diving into machine learning.    b. Discuss the importance of defining clear objectives, identifying relevant variables, and considering potential limitations\n",
            "or constraints.  6. Preprocessing (10 minutes)    a. Introduce the concept of data preprocessing and its role in preparing the data for machine learning algorithms.\n",
            "b. Discuss common preprocessing techniques, such as handling missing values, feature scaling, and encoding categorical variables.  7. Machine Learning Models (10\n",
            "minutes)    a. Provide an overview of popular machine learning models, such as linear regression, logistic regression, decision trees, random forests, and support\n",
            "vector machines.    b. Explain the strengths and weaknesses of each model and their suitability for different types of problems.  8. Evaluation Metrics (10 minutes)\n",
            "a. Introduce common evaluation metrics used to assess the performance of machine learning models, such as accuracy, precision, recall, and F1 score.    b. Discuss\n",
            "the importance of selecting appropriate evaluation metrics based on the problem and the desired outcome.  9. Conclusion and Q&A (5 minutes)    a. Summarize the key\n",
            "points covered in the lesson.    b. Encourage students to ask questions and clarify any doubts they may have.  Note: The duration of each section can be adjusted\n",
            "based on the students' understanding and engagement level.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-13\n",
        "#Using GPT-3.5-turbo model for pros and cons discusser\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "       {\"role\": \"user\", \"content\": \"Analyze the pros and cons of remote work vs. office work\"},\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luY5LmHDMtoi",
        "outputId": "04d68154-6c84-4280-8a91-e0ad1d91f4fb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pros of remote work: 1. Flexibility: Remote work allows employees to have more control over their work schedule and location. They can choose when and where they\n",
            "work, which can lead to a better work-life balance. 2. Increased productivity: Some studies suggest that remote workers are more productive than office workers. They\n",
            "often face fewer distractions and have the freedom to create a work environment that suits them best. 3. Cost savings: Remote work eliminates commuting costs, such\n",
            "as transportation expenses and parking fees. It can also reduce expenses related to work attire, meals, and office supplies. 4. Access to a global talent pool:\n",
            "Companies that embrace remote work can hire employees from anywhere in the world, expanding their talent pool and potentially finding highly skilled individuals who\n",
            "may not be available locally. 5. Environmental benefits: Remote work reduces carbon emissions by eliminating commuting. It also decreases the need for office space,\n",
            "leading to a smaller environmental footprint.  Cons of remote work: 1. Lack of face-to-face interaction: Remote work can lead to a sense of isolation and reduced\n",
            "social interaction. This can impact team collaboration, creativity, and the development of personal relationships among colleagues. 2. Communication challenges:\n",
            "Remote work relies heavily on digital communication tools, which may not always be as effective as in-person communication. Misunderstandings can occur, and it may\n",
            "be more difficult to build trust and rapport with colleagues. 3. Blurred work-life boundaries: Working from home can make it challenging to separate work and\n",
            "personal life. Without a clear distinction between the two, employees may find it difficult to disconnect and may experience burnout. 4. Limited access to resources:\n",
            "Remote workers may not have the same access to office resources, equipment, or technology as their in-office counterparts. This could potentially impact their\n",
            "productivity and ability to perform certain tasks. 5. Potential for decreased visibility and career growth: Remote workers may have less visibility within the\n",
            "organization, which could impact their chances of career advancement or being considered for certain opportunities.  Pros of office work: 1. Face-to-face\n",
            "collaboration: Office work allows for immediate and direct communication, fostering collaboration, brainstorming, and problem-solving among colleagues. It can lead\n",
            "to more efficient decision-making processes. 2. Social interaction and networking: Working in an office environment provides opportunities for socializing, building\n",
            "relationships, and networking with colleagues. This can enhance teamwork, employee engagement, and overall job satisfaction. 3. Access to resources and\n",
            "infrastructure: Office workers have access to office equipment, technology, and resources that may not be readily available to remote workers. This can facilitate\n",
            "productivity and efficiency in completing tasks. 4. Clear work-life boundaries: Having a physical separation between work and personal life can help individuals\n",
            "establish clear boundaries, allowing for better work-life balance and reducing the risk of burnout. 5. Increased visibility and career growth opportunities: Being\n",
            "physically present in the office can provide more visibility to managers and decision-makers, potentially leading to increased career growth opportunities and\n",
            "professional development.  Cons of office work: 1. Commuting and associated costs: Office workers often face long commutes, which can be time-consuming and\n",
            "expensive. Commuting can also contribute to stress and fatigue. 2. Distractions and interruptions: Office environments can be noisy and filled with distractions,\n",
            "such as co-workers' conversations or interruptions from colleagues. This can hinder concentration and productivity. 3. Limited flexibility: Office work typically\n",
            "follows a fixed schedule, offering less flexibility in terms of working hours and location. This can be challenging for individuals with personal or family\n",
            "commitments. 4. Higher expenses: Office workers may incur additional expenses, such as transportation costs, parking fees, and expenses related to work attire,\n",
            "meals, and office supplies. 5. Environmental impact: Office work contributes to carbon emissions through commuting and the energy consumption of office buildings. It\n",
            "also requires more physical space, leading to a larger environmental footprint.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-14\n",
        "#Using GPT-3.5-turbo model for review classifier\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "       {\"role\": \"system\", \"content\": \"You will be presented with user reviews and your job is to provide a set of tags from the following list. Provide your answer in bullet point form. Choose ONLY from the list of tags provided here (choose either the positive or the negative tag but NOT both):\\n    \\n    - Provides good value for the price OR Costs too much\\n    - Works better than expected OR Did not work as well as expected\\n    - Includes essential features OR Lacks essential features\\n    - Easy to use OR Difficult to use\\n    - High quality and durability OR Poor quality and durability\\n    - Easy and affordable to maintain or repair OR Difficult or costly to maintain or repair\\n    - Easy to transport OR Difficult to transport\\n    - Easy to store OR Difficult to store\\n    - Compatible with other devices or systems OR Not compatible with other devices or systems\\n    - Safe and user-friendly OR Unsafe or hazardous to use\\n    - Excellent customer support OR Poor customer support\\n    - Generous and comprehensive warranty OR Limited or insufficient warranty\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"I recently purchased the Inflatotron 2000 airbed for a camping trip and wanted to share my experience with others. Overall, I found the airbed to be a mixed bag with some positives and negatives.\\n    \\n    Starting with the positives, the Inflatotron 2000 is incredibly easy to set up and inflate. It comes with a built-in electric pump that quickly inflates the bed within a few minutes, which is a huge plus for anyone who wants to avoid the hassle of manually pumping up their airbed. The bed is also quite comfortable to sleep on and offers decent support for your back, which is a major plus if you have any issues with back pain.\\n    \\n    On the other hand, I did experience some negatives with the Inflatotron 2000. Firstly, I found that the airbed is not very durable and punctures easily. During my camping trip, the bed got punctured by a stray twig that had fallen on it, which was quite frustrating. Secondly, I noticed that the airbed tends to lose air overnight, which meant that I had to constantly re-inflate it every morning. This was a bit annoying as it disrupted my sleep and made me feel less rested in the morning.\\n    \\n    Another negative point is that the Inflatotron 2000 is quite heavy and bulky, which makes it difficult to transport and store. If you're planning on using this airbed for camping or other outdoor activities, you'll need to have a large enough vehicle to transport it and a decent amount of storage space to store it when not in use.\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLyX3X8iNxaZ",
        "outputId": "3a6781da-75fc-48cd-ec60-c57dd7419e83"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Easy to use - High quality and durability - Difficult to transport - Difficult to store\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-13\n",
        "#Using GPT-3.5-turbo model for Meeting notes summarizer\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "   {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You will be provided with meeting notes, and your task is to summarize the meeting as follows:\\n    \\n    -Overall summary of discussion\\n    -Action items (what needs to be done and who is doing it)\\n    -If applicable, a list of topics that need to be discussed more fully in the next meeting.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Meeting Date: March 5th, 2050\\n    Meeting Time: 2:00 PM\\n    Location: Conference Room 3B, Intergalactic Headquarters\\n    \\n    Attendees:\\n    - Captain Stardust\\n    - Dr. Quasar\\n    - Lady Nebula\\n    - Sir Supernova\\n    - Ms. Comet\\n    \\n    Meeting called to order by Captain Stardust at 2:05 PM\\n    \\n    1. Introductions and welcome to our newest team member, Ms. Comet\\n    \\n    2. Discussion of our recent mission to Planet Zog\\n    - Captain Stardust: \\\"Overall, a success, but communication with the Zogians was difficult. We need to improve our language skills.\\\"\\n    - Dr. Quasar: \\\"Agreed. I'll start working on a Zogian-English dictionary right away.\\\"\\n    - Lady Nebula: \\\"The Zogian food was out of this world, literally! We should consider having a Zogian food night on the ship.\\\"\\n    \\n    3. Addressing the space pirate issue in Sector 7\\n    - Sir Supernova: \\\"We need a better strategy for dealing with these pirates. They've already plundered three cargo ships this month.\\\"\\n    - Captain Stardust: \\\"I'll speak with Admiral Starbeam about increasing patrols in that area.\\n    - Dr. Quasar: \\\"I've been working on a new cloaking technology that could help our ships avoid detection by the pirates. I'll need a few more weeks to finalize the prototype.\\\"\\n    \\n    4. Review of the annual Intergalactic Bake-Off\\n    - Lady Nebula: \\\"I'm happy to report that our team placed second in the competition! Our Martian Mud Pie was a big hit!\\\"\\n    - Ms. Comet: \\\"Let's aim for first place next year. I have a secret recipe for Jupiter Jello that I think could be a winner.\\\"\\n    \\n    5. Planning for the upcoming charity fundraiser\\n    - Captain Stardust: \\\"We need some creative ideas for our booth at the Intergalactic Charity Bazaar.\\\"\\n    - Sir Supernova: \\\"How about a 'Dunk the Alien' game? We can have people throw water balloons at a volunteer dressed as an alien.\\\"\\n    - Dr. Quasar: \\\"I can set up a 'Name That Star' trivia game with prizes for the winners.\\\"\\n    - Lady Nebula: \\\"Great ideas, everyone. Let's start gathering the supplies and preparing the games.\\\"\\n    \\n    6. Upcoming team-building retreat\\n    - Ms. Comet: \\\"I would like to propose a team-building retreat to the Moon Resort and Spa. It's a great opportunity to bond and relax after our recent missions.\\\"\\n    - Captain Stardust: \\\"Sounds like a fantastic idea. I'll check the budget and see if we can make it happen.\\\"\\n    \\n    7. Next meeting agenda items\\n    - Update on the Zogian-English dictionary (Dr. Quasar)\\n    - Progress report on the cloaking technology (Dr. Quasar)\\n    - Results of increased patrols in Sector 7 (Captain Stardust)\\n    - Final preparations for the Intergalactic Charity Bazaar (All)\\n    \\n    Meeting adjourned at 3:15 PM. Next meeting scheduled for March 19th, 2050 at 2:00 PM in Conference Room 3B, Intergalactic Headquarters.\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7,frequency_penalty=0.5\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9fQkGqiQ5zz",
        "outputId": "a2cb6565-da8f-489e-f42c-9fc67ff3b763"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Summary of Discussion: The meeting began with introductions and a warm welcome to the newest team member, Ms. Comet. The team then discussed the recent\n",
            "mission to Planet Zog, acknowledging the success but also highlighting the need to improve language skills for better communication. Dr. Quasar volunteered to work\n",
            "on a Zogian-English dictionary. Lady Nebula suggested having a Zogian food night on the ship. The team then addressed the space pirate issue in Sector 7, with Sir\n",
            "Supernova expressing concern and Captain Stardust planning to speak with Admiral Starbeam about increasing patrols. Dr. Quasar mentioned working on a cloaking\n",
            "technology prototype to avoid detection by pirates. The team reviewed their performance in the annual Intergalactic Bake-Off, placing second with their Martian Mud\n",
            "Pie and aiming for first place next year with Ms. Comet's secret recipe for Jupiter Jello. Planning for an upcoming charity fundraiser was discussed, including ideas\n",
            "for games and booths at the Intergalactic Charity Bazaar. Lastly, Ms. Comet proposed a team-building retreat to the Moon Resort and Spa, which was well-received by\n",
            "Captain Stardust.  Action Items: - Dr. Quasar will work on a Zogian-English dictionary. - Captain Stardust will speak with Admiral Starbeam about increasing patrols\n",
            "in Sector 7. - Dr. Quasar will finalize the cloaking technology prototype. - All team members will gather supplies and prepare games for the Intergalactic Charity\n",
            "Bazaar.  Topics for Next Meeting: - Update on the progress of the Zogian-English dictionary (Dr. Quasar) - Progress report on the cloaking technology prototype (Dr.\n",
            "Quasar) - Results of increased patrols in Sector 7 (Captain Stardust) - Final preparations for the Intergalactic Charity Bazaar (All)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-14\n",
        "#Using GPT-3.5-turbo model for Natural Language to SQL\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "   {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"Given the following SQL tables, your job is to write queries given a user’s request.\\n    \\n    CREATE TABLE Orders (\\n      OrderID int,\\n      CustomerID int,\\n      OrderDate datetime,\\n      OrderTime varchar(8),\\n      PRIMARY KEY (OrderID)\\n    );\\n    \\n    CREATE TABLE OrderDetails (\\n      OrderDetailID int,\\n      OrderID int,\\n      ProductID int,\\n      Quantity int,\\n      PRIMARY KEY (OrderDetailID)\\n    );\\n    \\n    CREATE TABLE Products (\\n      ProductID int,\\n      ProductName varchar(50),\\n      Category varchar(50),\\n      UnitPrice decimal(10, 2),\\n      Stock int,\\n      PRIMARY KEY (ProductID)\\n    );\\n    \\n    CREATE TABLE Customers (\\n      CustomerID int,\\n      FirstName varchar(50),\\n      LastName varchar(50),\\n      Email varchar(100),\\n      Phone varchar(20),\\n      PRIMARY KEY (CustomerID)\\n    );\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Write a SQL query which computes the average total order value for all orders on 2023-04-01.\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW2WzFw1RU2_",
        "outputId": "df5d3d41-b428-4108-eebc-08ed1fed0e42"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT AVG(total_order_value) AS average_order_value FROM (   SELECT o.OrderID, SUM(p.UnitPrice * od.Quantity) AS total_order_value   FROM Orders o   INNER JOIN\n",
            "OrderDetails od ON o.OrderID = od.OrderID   INNER JOIN Products p ON od.ProductID = p.ProductID   WHERE o.OrderDate = '2023-04-01'   GROUP BY o.OrderID ) subquery;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-15\n",
        "#Using GPT-3.5-turbo model for Language translation\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "   {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You will be provided with a sentence in English, and your task is to translate it into German.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Languaage translation is the most booming topic in current situation\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAgpdSNlR78F",
        "outputId": "8f5468c0-ee19-43fa-8c9c-25291857fdde"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sprachübersetzung ist das derzeit boomendste Thema.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-16\n",
        "#Using GPT-3.5-turbo model for improving code efficiency\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "   {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You will be provided with a piece of Python code, and your task is to provide ideas for efficiency improvements.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"from typing import List\\n def has_sum_k(nums: List[int], k: int) -> bool:\\n        \\\"\\\"\\\"\\n        Returns True if there are two distinct elements in nums such that their sum \\n        is equal to k, and otherwise returns False.\\n        \\\"\\\"\\\"\\n        n = len(nums)\\n        for i in range(n):\\n            for j in range(i+1, n):\\n                if nums[i] + nums[j] == k:\\n                    return True\\n        return False\"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "#print(expanded_text)\n",
        "max_width = 165\n",
        "# Create a TextWrapper instance\n",
        "wrapper = textwrap.fill(expanded_text,width=max_width)\n",
        "print(wrapper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnl-Lt6sTUCW",
        "outputId": "78a8dbae-df00-40e6-d08c-a5357f2eeda9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One possible improvement is to use a set to store the complement of each number as we iterate through the list. This way, we can check if the complement exists in\n",
            "the set in constant time, resulting in a more efficient solution.  Here's an updated version of the code with this improvement:  ```python from typing import List\n",
            "def has_sum_k(nums: List[int], k: int) -> bool:     \"\"\"     Returns True if there are two distinct elements in nums such that their sum      is equal to k, and\n",
            "otherwise returns False.     \"\"\"     complements = set()     for num in nums:         complement = k - num         if complement in complements:             return\n",
            "True         complements.add(num)     return False ```  This updated code has a time complexity of O(n), where n is the length of the input list `nums`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-17\n",
        "#Using GPT-3.5-turbo model for writing a function from the specification\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"write a python code which takes 3 numbers as input and finds the greatest of them \"\n",
        "    }\n",
        "     ], temperature=0.3,top_p=0.7\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e5yw7fmU6jc",
        "outputId": "af3bbcad-9a0a-4e03-9be2-cefa92f03d1b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a Python code that takes 3 numbers as input and finds the greatest of them:\n",
            "\n",
            "```python\n",
            "# Taking input from the user\n",
            "num1 = float(input(\"Enter the first number: \"))\n",
            "num2 = float(input(\"Enter the second number: \"))\n",
            "num3 = float(input(\"Enter the third number: \"))\n",
            "\n",
            "# Comparing the numbers to find the greatest\n",
            "if num1 >= num2 and num1 >= num3:\n",
            "    greatest = num1\n",
            "elif num2 >= num1 and num2 >= num3:\n",
            "    greatest = num2\n",
            "else:\n",
            "    greatest = num3\n",
            "\n",
            "# Printing the greatest number\n",
            "print(\"The greatest number is:\", greatest)\n",
            "```\n",
            "\n",
            "In this code, we take three numbers as input from the user using the `input()` function. Then, we compare the numbers using if-elif-else statements to find the greatest among them. Finally, we print the greatest number using the `print()` function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-18\n",
        "#Using GPT-3.5-turbo model for text completion\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"Complete the text that will be provided to you\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Machine learning is a\"\n",
        "    }\n",
        "     ], temperature=0.8,top_p=0.1,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYb_wIaWV2mi",
        "outputId": "ab16fea3-d21d-49b8-fc47-50e3e355fdd3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subfield of artificial intelligence that focuses on the development of algorithms and models that allow computers to learn and make predictions or decisions without being explicitly programmed. It involves the use of statistical techniques and data analysis to enable machines to learn from and adapt to new information. Machine learning algorithms can be trained on large datasets to recognize patterns, make predictions, or classify data. This technology has a wide range of applications, including image and speech recognition, natural language processing, recommendation systems, and autonomous vehicles. As more data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-19\n",
        "#Using GPT-3.5-turbo model for information extraction\n",
        "#Import openai library and initialize openai client\n",
        "import textwrap\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "#Utilize the GPT-3.5-turbo model to generate a chat-based completion.\n",
        "#Additional fine tuning parameters used are\n",
        "#Temperature-Controls randomness of generated output. 0.8-1:More diverse output,0.2-0.5:deterministic output\n",
        "#Max_tokens-Max number of words in the output response\n",
        "#top_p-Way to control the diversity of the generated text by adjusting the threshold for the most probable tokens considered during sampling.\n",
        "#0.7 means the model considers only the most likely tokens whose cumulative probability mass is 70% or higher.\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You will be provided some text information and ask questions from the text provided\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Author-contribution statements and acknowledgements in research papers should state clearly and specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the preparation of their manuscript and analysis. They should also indicate which LLMs were used. This will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases, inaccuracies and improper source crediting. Likewise, scientific journals should be transparent about their use of LLMs, for example when selecting submitted manuscripts.Mention the large language model based product mentioned in the paragraph above:\"\n",
        "    }\n",
        "     ], temperature=0.8,top_p=0.1,max_tokens=100\n",
        ")\n",
        "#To retrieve and Print the response\n",
        "message=completion.choices[0].message\n",
        "expanded_text = message.content.expandtabs(tabsize=4)  # Adjust tab size as needed\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvC6KShviqAW",
        "outputId": "07b8889b-c43a-4b38-97c7-8eb2bdf1edb9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The large language model based product mentioned in the paragraph above is ChatGPT.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 20\n",
        "#Using completions API with the older versions of gpt models as they dont support chat completions. Chat completios is only available in gpt 3.5 and gpt4\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = openai.completions.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=\"Translate the following English text to French: '{Iam found of eating}'\",\n",
        ")\n",
        "text=response.choices[0].text\n",
        "print(text)"
      ],
      "metadata": {
        "id": "RJczswyZkUqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f53701-ff06-4e5c-afad-e49702ecd682"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Je suis fan de manger.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 21\n",
        "#Using completions API with the older versions of gpt models as they dont support chat completions. Chat completios is only available in gpt 3.5 and gpt4\n",
        "#Using parameters in completions method\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = openai.completions.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=\"Act as sudha murthy : '{Write a short story having only 500 words}'\",max_tokens=500,temperature=0.7,\n",
        ")\n",
        "text=response.choices[0].text\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23oqLHID07U8",
        "outputId": "c1ee421a-1e12-4471-898f-10accfdec047"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Once upon a time, there lived a boy named Ashok. He was a bright and hardworking student and had a lot of ambition in life. He wanted to make his parents proud and also make his own mark in the world.\n",
            "\n",
            "One day, Ashok's parents decided to take him to a nearby temple. It was a temple dedicated to Goddess Durga, the goddess of power, courage, and strength. Ashok was mesmerized by the beauty and grandeur of the temple. He had never seen anything like it before.\n",
            "\n",
            "Ashok prayed to the goddess and asked for her blessings. He asked for the strength to pursue his dreams and to make his parents proud. He promised that if she blessed him, he would always be true to his word and never give up.\n",
            "\n",
            "The goddess smiled upon him and granted his wish. She said that she would give him the power to make his dreams come true.\n",
            "\n",
            "A few months later, Ashok won a prestigious scholarship to study abroad. He was thrilled and his parents were overjoyed. His hard work and dedication had paid off.\n",
            "\n",
            "Ashok was determined to make the most of his opportunity. He worked hard and excelled in his studies. He made many friends and enjoyed his time there.\n",
            "\n",
            "However, he never forgot the promise he made to the goddess. He never gave up and kept his faith in her. He continued to work hard and achieved success.\n",
            "\n",
            "Today, Ashok is a successful businessman and has made his parents and the goddess proud. He visits the temple regularly and thanks her for her blessings.\n",
            "\n",
            "This story is a reminder that hard work and dedication can make all our dreams come true. We just need to have faith and never give up.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 22\n",
        "#Using completions API with the older versions of gpt models as they dont support chat completions. Chat completios is only available in gpt 3.5 and gpt4\n",
        "#Using parameters in completions method by asking the model to give 2 different answers\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = openai.completions.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=\"Interview questions for the job of a data analyst'\",max_tokens=500,temperature=0.7,n=2\n",
        ")\n",
        "output1=response.choices[0].text\n",
        "print(output1)\n",
        "output2=response.choices[1].text\n",
        "print(output2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMkjALqx17iu",
        "outputId": "7e114470-a7e2-4206-bb62-c3f354b28789"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. What experience do you have in working with data analysis?\n",
            "2. Describe the most complex data analysis project you have completed.\n",
            "3. What challenges have you faced while working with large datasets?\n",
            "4. How do you stay current with the latest trends and technologies in data analysis?\n",
            "5. How do you ensure accuracy when dealing with a large amount of data?\n",
            "6. What techniques do you use to extract data from different sources?\n",
            "7. Describe your experience with creating data visualizations.\n",
            "8. How do you test and validate data analysis results?\n",
            "9. What experience do you have in using various data analysis tools?\n",
            "10. How do you prioritize tasks when dealing with multiple projects?\n",
            "\n",
            "\n",
            "1. What experience do you have in analyzing data?\n",
            "2. How do you ensure the accuracy of your data analysis?\n",
            "3. Describe your experience with using various data analysis tools such as Excel, SPSS, SAS, etc.\n",
            "4. What techniques do you use to identify patterns and trends in data?\n",
            "5. How would you go about creating a data model?\n",
            "6. What challenges have you faced when working with large datasets?\n",
            "7. How do you stay up to date on new data analysis tools and techniques?\n",
            "8. What processes do you use to ensure data integrity?\n",
            "9. What strategies do you use to ensure the data you are analyzing is reliable?\n",
            "10. How do you handle conflicting data points?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b96kITu73qHG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNL8uQKJeftvOpHLLWaxBg3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}